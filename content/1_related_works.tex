\section{Related Work} \label{related_works}

\subsection{Limitations in Likert scale surveys}
The Likert scale is an intensity scale used to elicit participants' level of agreement, satisfaction, {\change{and perceived}} importance~\cite{likert1932technique}. Invented by psychologist Rensis Likert in 1932, the initial design of {\change{the}} Likert scale aimed to identify clusters of opinions within a crowd~\cite{joshi2015likert}. The initial Likert scale survey was a 5-point scale\footnote{The original design used: Strongly Approve (1), Approve (2), Undecided (3), Disapprove (4), Strongly Disapprove (5) as the five scales.}, and subsequent researchers 3, 7, 11, or 12-point Likert scale variations~\cite{garland2008computer,finstad2010}. Some researchers developed alternative forms of Likert scale interfaces such as slider scales~\cite{roster2015exploring} or phrase completions~\cite{hodge2003phrase} to improve usability of the traditional Likert scale survey. 

Likert scale surveys have grown in popularity since they are easy to understand and administer across domains. However, as a ratings approach, Likert scale has its limitations. The primary limitation is not able to accurately understand how participants' prioritize a set of options due to several response biases, a phenomenon where survey respondents' stated {\change{opinions do}} not align with their true opinions. 

Likert scale surveys suffer from ``acquiescence bias,'' a type of response bias where participants tend to select the same level across the entire survey~\cite{alwin1985measurement, moors2016two}. To minimize this bias, researchers typically design the same ratio of positively and negatively framed options~\cite{kuru2016improving}. 

Another type of bias, ``extreme responding,'' occurs when participants only answer on the extreme ends of the Likert scale~\cite{batchelor2016extreme, furnham1986response, meisenberg2008acquiescent}, misaligned with their true preferences. An empirical study by \textcite{quarfoot2017quadratic} observed this phenomenon --- participants either expressed polarized opinions or did not express an opinion at all, making it hard for survey creators to form an optimal strategy based on the survey responses~\cite{posner2018radical}. \textcite{cavaille2018towards} provided a theoretical explanation for this bias; respondents exaggerate their opinions on purpose to influence the outcome of the survey. Researchers have developed statistical methods and suggested best practices for better question designs to mitigate such biases~\cite{glaser2008response}.

Although researchers have designed various solutions to alleviate these response biases stemmed from the \textit{mechanism} of Likert scale, in this work, we examine QV as a potential alternative that is less prone to {\change{these response}} biases.

\subsection{Quadratic Voting}
\textcite{posner2018radical} developed Quadratic Voting (QV), a collective decision-making mechanism~\cite{lalley2018quadratic} to circumvent the tyranny of the majority in traditional one-person-one-vote mechanisms, where the majority favors one option over the rest, always limiting the voice of the minority. Since QV participants are not bound to a single vote, QV does not have such a concern. Inspired by the Vickrey-Clarke-Groves mechanism~\cite{roughgarden2010algorithmic}, {\change{in QV, the marginal cost to cast an additional vote grows proportionally to the votes already cast on that option, inducing rational participants to vote proportionally to how much they care about an issue~\cite{posner2018radical}. This design is why,}} unlike many traditional voting methods, each {\change{QV}} vote comes with a {\change{quadratic}} cost. 

Here we formally define QV. Consider collecting responses from $S$ participants, where each person has access to a fixed number of voice credits $B$ to allocate across options. Then, each {\change{person can cast more than one vote (for, or against) for each option, with the proviso that the votes have a quadratic cost. Thus, casting $n_k$ votes on option $k$ would cost $c(n_k) \propto n_k^2$ in voice credits. Furthermore, the total cost in voice credits across all options cannot exceed the budget $B$. Therefore, a person casting positive or negative votes across $k$ options has to satisfy $\sum_k n_k^2 \leqslant B$, where $n_k$ is the number of votes cast on option $k$. At last, survey creators analyze the aggregated results by comparing the total number of votes from all participants across each option. }}

Several works explored the theoretical properties of QV. \textcite{lalley2018quadratic} proved theoretically that QV's total welfare loss converges as the number of respondents increases. {\change{Similarly, a recent work by \textcite{eguia2019quadratic} theoretically proved that the probability of QV aggregates  reaching a socially efficient outcome converged to one as the number of participants increased in a resource-constrained survey. ~\textcite{Lalley2018} also proved \textit{robust optimality} properties of QV, suggesting that QV may be incentive-compatible (i.e., truth-telling is the dominant strategy for the respondent). Our study examines if QV can elicit incentive-compatible results}} from an empirical lens instead of a theoretical lens. We next discuss empirical studies that compared QV with Likert scale.

\subsection{Comparing QV with Likert Scale}
Since QV is a relatively new voting mechanism, we are only aware of two studies that empirically compared QV with Likert scale. Both of these studies focused on comparing the characteristics of responses from QV and Likert scale surveys.

\textcite{quarfoot2017quadratic} surveyed 4500 participants on their opinions for ten public policies; each participant completed either a Likert scale survey, a QV survey, or both.  The study found that, for the same group of participants, responses on any option from the QV survey followed a normal distribution while those from the Likert scale survey were heavily skewed or polarized into ``W-shaped'' distributions. Researchers also noticed that individuals deliberated their responses more in QV and revealed more fine-grained attitudes. Thus, the study concluded that QV provided a clearer picture of the crowd's opinions to policy-makers on polarized issues~\cite{quarfoot2017quadratic}. 

Even though the study showed that the Likert scale survey and the QV survey produced \textit{different} results, it did not compare the survey {\change{results}} to the participants' true preferences. Our study takes one step further and compares their degree of alignment with participants' true preferences. Besides, the study focused on controversial policies, such as ``same-sex marriage'', on which voters had a strong tendency to agree or disagree at the extremes. It's unclear how QV and Likert perform when survey options are less polarized, e.g., choosing one's favorite ice cream flavor. Our study analyzes this latter scenario.

In another empirical study, \textcite{naylor2017first} utilized QV for an educational research to understand students' opinions towards a list of factors that impacted their success at universities. Results showed that QV provided more insights than the Likert survey, such as distinguishing good-to-have factors from must-have ones. {\change{Again, our study}} differs from this work as we focus on {\change{comparing}} the accuracy of survey responses.

Overall, to the best of our knowledge, prior work {\change{has neither}} studied the degree of alignment between QV responses and participants' true preferences {\change{nor compared}} it with the degree of alignment between Likert responses and true preferences. Therefore, our work is the first to examine QV's ability to elicit true preferences.
