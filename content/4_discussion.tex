\section{Discussion} \label{discussion}
In this section, we discuss the implications of the two experiment results. We first discuss when and why QV aligns better with true preferences and then discuss the impact of the QV voice credit budget. We conclude the section with a discussion around when to use QV. 

\subsection{When and Why Does QV Align Better with True Preferences?}
In both RQ1 and RQ2, we ask how QV survey responses align with people's true preferences compared to Likert scale survey responses in resource-constrained collective decision-making. From the two experiments, we showed that QV aligned significantly better with people's true preferences than a 5-point Likert scale survey when the survey respondents were asked to either (1) choose among $K$ independent options of the same topic or (2) choose among $K$ dependent options that jointly contributed to the same topic. Note that the above conclusion was only true when the QV survey provided a medium ($O(K^{1.5})$) or large ($O(K^{2})$) voice credit budget to participants. Our Bayesian analysis showed a medium to high effect size for the difference between the degree of alignment in the QV group and the Likert group in both experiments. This suggests that QV had the potential to elicit true preferences more accurately in collective decision-making. We now discuss two potential reasons that may explain our results.

\textbf{Costs and scarcity}
One explanation for QV's better alignment in the above conditions may be the inherent difference between the cost of expressing an opinion in a QV and a Likert scale survey. Expressing opinions using a Likert scale does not carry any cost -- an individual freely selects any value, including extreme values, for all $K$ options if they prefer. In QV, participants pay for their votes for each option at a quadratic cost under a limited budget. 

Participants face a \textit{scarcity} of resources when they pay for costs with a limited budget. In behavioral economics, \textcite{Shah2015a} concluded in their work, ``Scarcity Frames Value'', that individuals would focus on what is necessary and become more rational in decision-making under resource constraints. They found that lower-income participants were less susceptible to framing effects than higher-income participants, likely because lower-income participants faced stricter resource constraints and thus were better at trade-off thinking. 

In conditions where a survey aims to understand how people make trade-offs among $K$ options, as in our two experiments, the concept of scarcity built into the QV mechanism may nudge participants to \textit{rank} the options and help participants make more rational decisions. We observed qualitative supports for this claim from experiment one. After experiencing a drop in the number of voice credits, one participant commented, ``I had fewer credits, so each vote seemed more expensive.'' This comment suggested that participants experienced the idea of ``scarceness'' and resource constraint via the limited voice credit budget during the QV survey. While scarcity prompts trade-off thinking, having too much scarcity in QV also limits participants' flexibility of expression, a possible reason for why QV36 underperformed QV108 and QV324 in experiment one. We discuss this issue further in the next two subsections.

\textbf{Flexibility of expression}
Another potential reason why QV aligned better with incentive-compatible behaviors is that, with a large enough budget, QV allowed participants more flexibility to express their opinions. A 5-point Likert scale provides only five choices for each question, limiting the way a participant can voice their opinion. One Likert group participant in the first experiment explicitly mentioned ``[\textellipsis] I would answer otherwise, if there were other options, such as not much, or a little bit.'' Participants wanted to express more fine-grained attitudes while a 5-point Likert scale forced them to map their preferences onto a limited fixed scale. QV, in contrast, allows participants to specify the relative distance between two options with greater flexibility, as long as the total cost does not exceed the given credits. The flexibility may enable participants to stay closer to their incentive-compatible preferences when rating the options.

We compared QV to a 5-point Likert scale in this study because the 5-point Likert scale is one of the most commonly used Likert scales in various fields \cite{malhotra2006basic}, including public policy and HCI. One may conjecture that a 7-point or 11-point Likert scale may allow more flexibility than a 5-point Likert scale. Debates about the scale format in Likert scale surveys started in 1965 \cite{komorita1965number}. Some studies found that results among different scale formats were transferable while others found certain differences \cite{dawes2008data}. We leave the comparison of Likert scale surveys in other scale formats and QV surveys as an open question for future research.
% This research showed that if the options are homogeneous, a two-point Likert scale yields as high-reliability coefficient as a multi-category system. The research also emphasized that reliability should not be the only metric when deciding which scale is better than the other scale, much more the opposite. It implies that a scale should be chosen for the purpose it aims to serve and the context it is used in. However, in modern research that utilize Likert surveys, there was often little to no discussions describing why the researchers used a specific type of Likert survey. 

\subsection{Effects of the Amount of Voice Credits}
In RQ1, we investigated how the number of voice credits available to participants impacted the QV survey results empirically. In experiment one, Bayesian analysis showed that QV results did not align with true preferences significantly better than a 5-point Likert scale until a sufficient amount of voice credits was used. We saw that QV with 36 voice credits ($O(K)$) under-performed QV with 108 ($O(K^{1.5})$) and 324 ($O(K^{2})$) voice credits. We found potential explanations through participants' free-form text responses. 

When participants had only 36 credits in QV, some of them voiced their need to make hard trade-offs. \texttt{P9e5e6} said, ``I think I covered the bare basics.'' and \texttt{Pe37f2} said, ``Less to go around, so had to knuckle down and allocate the most to what I think is most important.'' These responses indicate that while extreme scarcity still encouraged participants \textit{ranking} behavior, it also limited their flexibility in expressing their degree of preferences, i.e. \textit{rating} the options.

On the other hand, when participants experienced an increase in voice credits from having only 36 credits, some of them expressed their appreciation for the increased freedom to state their opinions. \texttt{Pcc4aa} reported, ``with more credits I can show what I really like.'' and \texttt{P2d9da} stated, ``Because now that I have a lot more credits, I felt that I could vote on more issues that mean something to me.'' The different qualitative responses for QV36, QV108 and QV324 explain why QV with more voice credits performed better than QV36 in the first experiment. These qualitative responses also supports the concept that a higher level of flexibility may contribute to why QV outperformed a 5-point Likert scale in the degree of alignment in the previous subsection. 

While having fewer voice credits may worsen QV's degree of alignment with participants' incentive-compatible preferences, QV with a stringent budget may better elicit the options participants value most since it encourages harder trade-offs, based on the qualitative responses above. Future research could explore this potential effect of QV more closely.

In the first experiment, we explored up to a budget of $O(K^{2})$ voice credits, where $K$ is the number of options in the survey, specifically 324 credits. While QV aligned better than Likert scale up to this amount of voice credits and the percentage of credits used remained high (with a median of 98\%), we suspect that too many voice credits may pull participants away from trade-off thinking, or create excessive cognitive loads to participants. Where the threshold of ``too many'' voice credits lies remains an open question for future work.

\subsection{When to Use QV?}
Even though one may be tempted to conclude from this study that decision-makers should prefer QV to Likert in all circumstances, the goal of our study is not to claim that one survey method should replace another. QV has its strengths, and also weaknesses. 

Though QV elicits true preferences more accurately, it is less known among the crowd and there is a learning curve. While computational power enables survey respondents to perform rankings and ratings at the same time in QV, in scenarios where only paper and phone-based surveys are possible, respondents may experience cognitive overload from the complicated calculation. Even though QV surveys are great at informing decisions with trade-offs (e.g., what desserts do you prefer tonight), it may not work for questions that do not imply trade-offs (e.g., what desserts do you like -- I like all desserts!). Therefore, survey creators should carefully consider the pros and cons of QV and select the best suiting survey method in their contexts.

\section{Limitations and Future Work}
QV is a relatively new area of research. Comparing the alignment of Likert scale and QV with users' true preferences is challenging. During the study, we identified various open questions that we've yet to address. In this subsection, we address our limitations and propose open questions for future research. 


\subsection{Comparing Ordinal Data with Numerical Data}
To compare ordinal Likert scale responses with numerical donation amounts or set prices, we mapped the 5-point Likert scale to integers in the range of $[-2, 2]$. We used the number of votes in QV directly since they are numerical. We made this decision because selecting "Neutral" (mapped value = 0) in the Likert scale had a similar meaning as casting a zero vote in QV.

Whether our approach of mapping Likert data to metric values is the best approach to compare ordinal data with numerical data is debatable. At the same time, identifying the best measure to do so is challenging -- the best way to analyze ordinal Likert data is still open to debate today \cite{gob2007ordinal}. Future research could explore if there are alternatives for such a comparison that circumvent the challenge of mapping ordinal data. 

\subsection{Comparing QV with Other Surveying Methods}
% In the previous subsection, we mentioned that Likert survey choices could limit individuals' degree of freedom to express their attitudes. One open question includes investigating if another type of Likert survey influences participants' behavior compared to QV. In addition,
Despite Likert scale being one of the most-used surveying techniques, we are curious about how other voting mechanisms that capture the concept of resource constraints or relative-preference elicitation perform compared to QV. Examples include but are not limited to knapsack voting \cite{goel2015knapsack} and ranked-based voting \cite{ledo2018evaluation}. Knapsack voting similarly uses a budget, but the cost of QV's vote grows quadratically while the cost of an option in knapsack voting is fixed. Ranked-based voting elicits relative rankings among options, which incurs trade-off thinking, but does not show the magnitude of differences between the options, unlike QV. The coexistence of differences and similarities of these voting mechanisms with QV makes the comparison of their performances an interesting open question.

\subsection{Upper Bound of the Number of Options}

In experiment one, our participants chose between 9 options, while in our second experiment, participants had 5 options on the survey. In both cases, QV performed well, suggesting that participants could make effective trade-offs in QV across up to 9 options. However, our study did not identify the upper bound of the number of options users can handle comfortably on a QV survey. One can imagine the difficulty for QV survey respondents to vote among dozens of survey options. In fact, work by Iyengar et al. \cite{iyengar2000choice} observed that more choices may not necessarily increase participants' satisfaction, suggesting that people were not good at making choices across an extensive array of options. The same phenomenon could happen in our case -- is there a limit to how many options could be on a QV survey to maintain high-quality data collection?

\subsection{Generalizability to Different Types of Surveys}
In this study, we examined QV in two settings. We chose settings that made sense to translate into QV surveys and leveraged prior research. We did not exhaustively examine the type of survey questions that work with QV and those that may not. Hence, readers should take caution in generalizing our results to other survey settings.

In the first experiment, the survey asked participants to choose among $K$ independent options for the same topic. In experiment two, the survey asked participants to choose among $K$ dependent aspects that jointly contribute to the same topic. Though different, both of these surveys aimed to help us understand relative preferences and trade-offs. We do not yet know if QV would work for a survey with a different relationship between survey options, e.g., surveys that consist of options that are not on the same topic, or a survey with a different goal, e.g., surveys that do not involve any resource constraint.

Similarly, our study only tested QV in the context of public policy and an HCI user study. Many other disciplines make use of Likert scale surveys to help make collective decisions. Future research can explore if QV better elicits true preferences than Likert scale in other domains.

\lwt{
\subsection{Generalizability to Non-US Population}
While our experiment samples covered a wide range of ages and education levels, we targeted only the US population due to limited resources. Prior studies have found that cultural background affected response patterns in Likert scale surveys \cite{davey2007one}. Thus, future research needs to explore if cultural background also impacts people's interaction with QV and whether our results hold under those circumstances.
}

\subsection{User Study on QV}
Understanding how individuals learn about, feel about, and use QV is an important topic. Without a doubt, understanding how QV works requires more cognitive load than traditional voting and surveying techniques, and using QV takes more time and effort. In addition, examining respondents' mental models of QV and how the models impact individuals decision-making process may help decision-makers further understand the effectiveness of QV empirically and design better QV surveys and interfaces. Our study only scratched the surface of the above questions by using the responses from a few free-form text questions. Thus, future work may conduct a rigorous user study to explore these questions.

\subsection{Interface Design for QV}
The final open question is designing a simple, intuitive QV interface for empirical use. QV involves more complicated calculations than Likert. A well-designed interface should reduce a user's cognitive load to help them make accurate decisions easily. Currently, after our iterative-design process, we provided participants information such as the number of votes per option, voice credits used and voice credits remaining, and how they allocate the voice credits to each option. 

Different interface designs could nudge users to behave in specific ways. How the interface should provide voters with this information in an optimal way remains an open question. Finally, we need to investigate QV interface designs for mobile and tablet devices.
% Besides, showing an individual how they had allocated their votes could also potentially interfere and encourage voters to vote for or against an option

\tc{
\subsection{Donation as True Preferences}
While abundant prior work used binding voluntary donations with real monetary consequences to elicit participants' true preferences \cite{zawojska2015re, getzner2000hypothetical, ready2010using, benz2008people, gendall2010effect}, we acknowledge the limitations of such approach, especially when we performed it with MTurk workers. Besides an individual's true preferences for a social cause, other factors may also impact their donation amount to a specific organization for the cause. The prior impression of an organization could be an important factor. To address this, we surveyed the participant's perception of these organizations and found no significant confound between their donation behavior and their responses (see Appendix X.) Another limitation is whether donation results on MTurk could be a reasonable estimation of MTurk workers' true preferences. A reasonable suspicion is that MTurk workers were less likely to donate in an MTurk task than in a real-life donation setting due to their stronger incentives to earn money. Based on an assumption that MTurk workers would reduce their donation amount equally across all causes, we minimized this limitation by focusing on the proportion of donations across the charities instead of using the absolute donation value. In other words, we were looking at how much one was willing to donate to a charity \textit{relative} to the other organizations.}

\subsection{Quality of Data Collection via MTurk}
Our experiments, like all other experiments conducted on MTurk, suffered from the limitation that not all participants joined the study with good faith or participated in the incentive-compatible tasks with a rational mindset.. To mitigate the risk, we implemented multiple tutorials, quizzes, and attention checks, filtered out responses with facetious free-form text responses, and introduced incentive-compatible tasks with financial incentives. \lwt{We have also confirmed that most participants used up almost all QV budgets and donated to more than one charities as shown in Appendix xxx.}



%We showed promising results of QV in our study -- QV elicited true preferences more accurately from participants than a 5-point Likert scale when making a collective decision to choose among $K$ options. Given the popularity of Likert scale surveys in a wide range of disciplines that involve self-reporting, our research asks an intriguing question -- whether there is an alternative survey method that can elicit accurate opinions by leveraging computational power. However, it is critical to reaffirm that the goal of this study is not to claim one survey method should replace another. As mentioned above, there still exist many open questions to understand the nuances in QV. Survey creators should carefully consider the strengths and weaknesses (e.g., higher cost to educate participants, higher cognitive cost for participants) of QV and select the best suiting survey method in their contexts.



% \subsection{Where are QV's limitations}
% Experiment two shows a slightly different and more nuanced result.
% It is important to know that
% the characteristics of the ballot options 
% are different compared to experiment one.
% On this ballot, the options are \textit{multiple aspects}
% of a same question.
% In other words, 
% one requires all options 
% to form the result of the final outcome.
% Let us use an analogy:
% If experiment one asks you
% ``how much do you prefer between
% coke, fries, and burgers'',
% the second experiment asks you
% ``how important do you think
% freshness, taste, and texture 
% of your beef patty are?''
% Notice the options in the first question 
% are independent while 
% those in the second question 
% collectively decide how good a beef patty is.

% From an intensity perspective 
% of the aggregated opinions, 
% qualitatively we can observe that 
% the preference result from QV aligns closer 
% to the true preference than that from Likert. 
% We have yet to examine its statistical significance 
% due to an experiment design limitation. 
% From a preference ranking perspective, 
% neither of the results from the Likert survey 
% and QV survey
% diverged significantly 
% from the incentive-compatible behaviors 
% that the buyback group demonstrated. 
% However, part of the Likert results 
% did diverge significantly from the QV results, 
% since aspects in Likert results clustered 
% in the higher rankings, 
% while aspects in the QV results 
% spread out more across the lower rankings. 
% Such finding echoes part of the conclusion 
% in the first experiment, 
% where QV is able to show finer-grained preference results 
% than Likert.

% In conclusion,
% we can conclude that QV with a sufficient budget outperforms Likert surveys when the survey aims to elicit preferences in a 1 in $K$ setting.
% There is not enough statistical evidence yet to conclude whether QV aligns to true preference better than Likert if the survey options are multiple aspects of the same subject. Nevertheless, both experiments demonstrated that QV elicits finer grain preferences from people than Likert in both cases, which is an advantage survey designer could leverage to gain more in-depth understanding of people's opinions.



% \subsection{Design Implications}
% Given these discussions, 
% we propose the following design implications. 
% First, QV provides more fine-grain results,
% including the preference and the level of intensity,
% a group has on a particular topic,
% when deciding among one in $K$ options.
% In the CSCW community,
% we believe this can be applied to 
% many collective decision-making processes.
% Form electing a great project among many,
% to redistributing limited resources among those in need,
% QV serves as an alternative tool 
% than traditional Likert surveys.
