\section{Discussion} \label{discussion}
In this section, we discuss the implications of the two experiment results. We first discuss when and why QV aligns better with true preferences and then discuss the impact of the QV voice credit budget. We conclude the section with a discussion around when to use QV. 

\subsection{When and Why Does QV Align Better with True Preferences?}
In both RQ1 and RQ2, we ask how QV survey responses align with people's true preferences compared to Likert scale survey responses in resource-constrained collective decision-making. From the two experiments, we showed that QV aligned significantly better with people's true preferences than a 5-point Likert scale survey when the survey respondents were asked to either (1) choose among $K$ independent options of the same topic or (2) choose among $K$ dependent options that jointly contributed to the same topic. Note that the above conclusion was only true when the QV survey provided a medium ($O(K^{1.5})$) or large ($O(K^{2})$) voice credit budget to participants. Our Bayesian analysis showed a medium to high effect size for the difference between the degree of alignment in the QV group and the Likert group in both experiments. This suggests that QV had the potential to elicit true preferences more accurately in collective decision-making. We now discuss two potential reasons that may explain our results.

\textbf{Costs and scarcity}
One explanation for QV's better alignment in the above conditions may be the inherent difference between the cost of expressing an opinion in a QV and a Likert scale survey. Expressing opinions using a Likert scale does not carry any cost---an individual freely selects any value, including extreme values, for all $K$ options if they prefer. In QV, participants pay for their votes for each option at a quadratic cost under a limited budget. 

Participants in our QV experiments face a \textit{scarcity} of resources when they pay for options via voice credits while working with a limited budget. \textcite{Shah2015a}, in their work, ``Scarcity Frames Value'', found that individuals making a decision under a scarcity constraint more readily perform trade-offs among the choices and were apt to ignore contextual cues, thus becoming more consistent with rational behavior. In conditions where a survey aims to understand how people make trade-offs among $K$ options, as we do in our two experiments, the QV mechanism with the emphasis of a limited budget may make the trade-off heuristic more accessible to participants. Thus QV may nudge participants to \textit{trade-off} among the options and help participants make more rational decisions. We observed qualitative supports for this claim from experiment one. After experiencing a drop in the number of voice credits, one participant commented, ``I had fewer credits, so each vote seemed more expensive.'' This comment suggested that participants experienced the idea of ``scarceness'' and resource constraint via the limited voice credit budget during the QV survey. While scarcity prompts trade-off thinking, having too much scarcity in QV also limits participants' flexibility of expression, a possible reason for why QV36 underperformed QV108 and QV324 in experiment one. We discuss this issue further in the next two subsections.

\textbf{Flexibility of expression}
Another potential reason why QV aligned better with incentive-compatible behaviors is that, with a large enough budget, QV allowed participants more flexibility to express their opinions. A 5-point Likert scale provides only five choices for each question, limiting the way a participant can voice their opinion. One Likert group participant in the first experiment explicitly mentioned ``[\textellipsis] I would answer otherwise, if there were other options, such as not much, or a little bit.'' Participants wanted to express more fine-grained attitudes while a 5-point Likert scale forced them to map their preferences onto a limited fixed scale. QV, in contrast, allows participants to specify the relative distance between two options with greater flexibility, as long as the total cost does not exceed the given credits. The flexibility may enable participants to stay closer to their incentive-compatible preferences when rating the options.

We compared QV to a 5-point Likert scale in this study because the 5-point Likert scale is one of the most commonly used Likert scales in various fields \cite{malhotra2006basic}, including public policy and HCI. One may conjecture that a 7-point or 11-point Likert scale may allow more flexibility than a 5-point Likert scale. Debates about the scale format in Likert scale surveys started in 1965 \cite{komorita1965number}. Some studies found that results among different scale formats were transferable while others found certain differences \cite{dawes2008data}. We leave the comparison of Likert scale surveys in other scale formats and QV surveys as an open question for future research.

\subsection{Effects of the Amount of Voice Credits}
In RQ1, we investigated how the number of voice credits available to participants impacted the QV survey results empirically. In experiment one, Bayesian analysis showed that QV results did not align with true preferences significantly better than a 5-point Likert scale until a sufficient amount of voice credits was used. We saw that QV with 36 voice credits ($O(K)$) under-performed QV with 108 ($O(K^{1.5})$) and 324 ($O(K^{2})$) voice credits. We found potential explanations through participants' free-form text responses. 

When participants had only 36 credits in QV, some of them voiced their need to make hard trade-offs. \texttt{P9e5e6} said, ``I think I covered the bare basics.'' and \texttt{Pe37f2} said, ``Less to go around, so had to knuckle down and allocate the most to what I think is most important.'' These responses indicate that while extreme scarcity still encouraged participants \textit{ranking} behavior, it also limited their flexibility in expressing their degree of preferences, i.e. \textit{rating} the options.

On the other hand, when participants experienced an increase in voice credits from having only 36 credits, some of them expressed their appreciation for the increased freedom to state their opinions. \texttt{Pcc4aa} reported, ``with more credits I can show what I really like.'' and \texttt{P2d9da} stated, ``Because now that I have a lot more credits, I felt that I could vote on more issues that mean something to me.'' The different qualitative responses for QV36, QV108 and QV324 explain why QV with more voice credits performed better than QV36 in the first experiment. These qualitative responses also supports the concept that a higher level of flexibility may contribute to why QV outperformed a 5-point Likert scale in the degree of alignment in the previous subsection. 

While having fewer voice credits may worsen QV's degree of alignment with participants' incentive-compatible preferences, QV with a stringent budget may better elicit the options participants value most since it encourages harder trade-offs, based on the qualitative responses above. Future research could explore this potential effect of QV more closely.

In the first experiment, we explored up to a budget of $O(K^{2})$ voice credits, where $K$ is the number of options in the survey, specifically 324 credits. While QV aligned better than Likert scale up to this amount of voice credits and the percentage of credits used remained high (with a median of 98\%), we suspect that too many voice credits may pull participants away from trade-off thinking, or create excessive cognitive loads to participants. Where the threshold of ``too many'' voice credits lies remains an open question for future work.

\subsection{When to Use QV?}
Even though one may be tempted to conclude from this study that decision-makers should prefer QV to Likert in all circumstances, the goal of our study is not to claim that one survey method should replace another. QV has its strengths, and also weaknesses. 

{\change{Though QV better elicits true preferences in comparison to Likert in the two experimental contexts in this paper, QV requires a learning curve for respondents. QV is suited for online surveys when respondents have the time to familiarize themselves with QV and are likely to use their smartphones, tablets, or personal computers to respond to the survey. QV is not well suited for surveys that allow respondents to fill out their responses on paper or survey respondents via telephone. Therefore, survey creators should carefully consider the pros and cons of QV and select the best suiting survey method in their contexts.}}

\section{Limitations and Future Work}
QV is a relatively new area of research. Comparing the alignment of Likert scale and QV with users' true preferences is challenging. During the study, we identified various open questions that we've yet to address. In this subsection, we address our limitations and propose open questions for future research. 

\subsection{Comparing Ordinal Data with Numerical Data}
To compare ordinal Likert scale responses with numerical donation amounts or set prices, we mapped the 5-point Likert scale to integers in the range of $[-2, 2]$. We used the number of votes in QV directly since they are numerical. We made this decision because selecting ``Neutral'' (mapped value = 0) in the Likert scale had a similar meaning as casting a zero vote in QV.

Whether our approach of mapping Likert data to metric values is the best approach to compare ordinal data with numerical data is debatable. At the same time, identifying the best measure to do so is challenging---the best way to analyze ordinal Likert data is still open to debate~\cite{gob2007ordinal}. Future research could explore if there are alternatives for such a comparison that circumvent the challenge of mapping ordinal data. 

\subsection{Comparing QV with Other Surveying Methods}

Despite the Likert scale being one of the most-used surveying techniques, one may be curious about how other voting mechanisms that capture the concept of resource constraints or make the trade-off heuristic accessible compare to QV. {\change{For example, voting algorithms in participatory budgeting (PB)~\cite{cabannes2004participatory,goel2015knapsack,Goel2016, Lee2014, benade2020preference}, used by governments to ask citizens to prioritize resource allocation, involve resource constraints. In participatory budgeting, voters are asked to identify a subset $I$ among projects $P$ on which they would like the government to allocate resources. Furthermore, each project $i \in P$ has a fixed implementation cost $c_i$ known to the survey taker. The survey-taker knows the total budget available $B$ to the decision-maker (e.g., local city council) and each respondent picks a subset $I \in P$ of projects such that the total costs $\sum_{i \in I} c_i \leqslant B$ stays within the budget. Examples of PB algorithms include knapsack voting~\cite{goel2015knapsack} and ranked voting ~\cite{ledo2018evaluation}.

PB algorithms elicit relative rankings among options, which incurs trade-off thinking like QV, but cannot elicit the \textit{strength} of a participant's preferences, i.e., the degree to which the participant prefers option A over B. PB algorithms that involve implementation costs, such as knapsack voting, may be less easy to adapt to CSCW or social science surveys, since the decision-maker, who creates the survey, has to assign implementation costs to each option and an overall budget, which may be problematic. For example, in a survey about interface design, the decision-maker would need to assign costs to each interface element on which they are eliciting an opinion. Furthermore, unlike the typical participatory budgeting scenario, where project costs involve allocating \textit{the respondent's tax dollars,} in typical social science and CSCW surveys, the implementation of an option involves no obvious cost to a participant.

An alternative option to knapsack voting that avoids the problem of cost assignment might be to use a linear constraint on the vote magnitudes---that is use $\sum_k |n_k| \leqslant B$, where $|n_k|$ is the magnitude of the vote for option $k$. In this case, the cost of an additional vote is constant, while that of QV increases proportionally with the vote already cast on that option. Comparing the performance of above mechanisms with QV makes an interesting open question.}}


\subsection{Upper Bound of the Number of Options}

In experiment one, our participants chose between 9 options, while in our second experiment, participants had 5 options on the survey. In both cases, QV performed well, suggesting that participants could make effective trade-offs in QV across up to 9 options. However, our study did not identify the upper bound of the number of options users can handle comfortably on a QV survey. One can imagine the difficulty for QV survey respondents to vote among dozens of survey options. In fact, work by Iyengar et al. \cite{iyengar2000choice} observed that more choices may not necessarily increase participants' satisfaction, suggesting that people were not good at making choices across an extensive array of options. The same phenomenon could happen in our case---is there a limit to how many options could be on a QV survey to maintain high-quality data collection?

\subsection{Generalizability to Different Types of Surveys}
In this study, we examined QV in two settings. We chose settings that made sense to translate into QV surveys and leveraged prior research. We did not exhaustively examine the type of survey questions that work with QV and those that may not. Hence, readers should take caution in generalizing our results to other survey settings.

{\change{We limited our experiments within the scope of resource-constrained surveys.}} In the first experiment, the survey asked participants to choose among $K$ independent options for the same topic. In experiment two, the survey asked participants to choose among $K$ dependent aspects that jointly contribute to the same topic. Though different, both of these surveys aimed to help us understand relative preferences and trade-offs. We do not yet know if QV would work for a survey in which survey options have a different relationship (e.g., surveys that consist of options that are not on the same topic), or a survey that do not involve any resource constraint.

Similarly, our study only tested QV in the context of public policy and an HCI user study. Many other disciplines make use of Likert scale surveys to help make {\change{resource-constrained}} decisions. Future research can explore if QV better elicits true preferences than Likert scale in other domains.



\subsection{User Study on QV}
Understanding how individuals learn about, feel about, and use QV is an important topic. Without a doubt, understanding how QV works requires more cognitive load than traditional voting and surveying techniques, and using QV takes more time and effort. In addition, examining respondents' mental models of QV and how the models impact individuals decision-making process may help decision-makers further understand the effectiveness of QV empirically and design better QV surveys and interfaces. Our study only scratched the surface of the above questions by using the responses from a few free-form text questions. Thus, future work may conduct a rigorous user study to explore these questions.

\subsection{Interface Design for QV}
The final open question is designing a simple, intuitive QV interface for empirical use. QV involves more complicated calculations than Likert. A well-designed interface should reduce a user's cognitive load to help them make accurate decisions easily. Currently, after our iterative-design process, we provided participants information such as the number of votes per option, voice credits used and voice credits remaining, and how they allocate the voice credits to each option. 

Different interface designs could nudge users to behave in specific ways. How the interface should provide voters with this information in an optimal way remains an open question. Finally, we need to investigate QV interface designs for mobile and tablet devices.

{\change{
\subsection{Donation as True Preferences}
That individuals donate to public goods, including charities, is a puzzle to economists since game theory predicts that rational individuals will free-ride, and thus there ought to be no contributions to public goods, including charities. Much of the experimental work in behavioral economics on public goods, as summarized by~\textcite{Fehr2007}, indicates that about 30\% of the participants in these experiments were free-riders---they \textit{do not} contribute to public goods. This is consistent with our own experimental finding where 27\% of the participants (see~\Cref{fig:total_don_exp1}) \textit{did not} contribute to public goods (i.e., charities) across all experimental groups.~\textcite{Andreoni1989} further developed the theory of ``impure altruism'' that involved ``warm-glow giving'', which suggests that individuals donate to public-goods in order to receive social acclaim or to avoid scorn, and is consistent with empirical observations of charitable giving. Thus the theory of `impure altruism' may explain the donations across charities in our experiment.

While prior work~\cite{zawojska2015re, getzner2000hypothetical, ready2010using, benz2008people, gendall2010effect} has used binding voluntary donations with real monetary consequences to elicit participants' incentive-compatible preferences, we acknowledge the limitations of such an approach. For example, factors, including the prior experience with a charity, may influence the donation amount. To check for possible bias, we first surveyed how each participant viewed (either favorably, neutral view or dis-favorably) the charitable organizations used in our experiments, prior to completing their donation tasks, to ensure that there was no systematic bias. All experimental groups exhibited similar favorability distributions across charities.

One other possible confound is that MTurkers may be less likely to donate since they have a strong incentive to earn money.  Thus, we might expect little or no donation from MTurkers. Assuming that MTurk workers reduce their donation amount equally across all causes, we minimized this limitation by focusing on the \textit{proportion} of donations across the charities instead of using the absolute donation value. In other words, we were looking at how much more one was willing to donate to a charity \textit{relative} to the other organizations.
}}

\subsection{Quality of Data Collection via MTurk}
Our experiments, like all other experiments conducted on MTurk, suffered {\change{from the limitation that not all participants joined the study with good faith or participated in the incentive-compatible tasks with a rational mindset.}} To mitigate the risk, we implemented multiple tutorials, quizzes, and attention checks, filtered out responses with facetious free-form text responses, and introduced incentive-compatible tasks with financial incentives. {\change{To ensure response quality, we confirmed that most participants used up almost all QV budgets and donated to more than one charities as shown in~\Cref{total_donation} and~\Cref{individual_donation}.}}

{\change{
\subsection{Generalizability to Non-US Population}
While our experiment samples covered a wide range of ages and education levels, we targeted only the US population due to limited resources. Prior studies have found that cultural background affected response patterns in Likert scale surveys \cite{davey2007one}. Thus, future research needs to explore if cultural background also impacts people's interaction with QV and whether our results hold under those circumstances.
}}