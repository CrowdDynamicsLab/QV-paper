\section{Discussion} \label{discussion}
In this section, we discuss implications from the two experiment results. In addition, we lay out many open questions discovered throughout this experiment.

% - merge disucssion with future work
% - Does QV align better with true preferences
%   -- draw connects to other works, behavioral economics
%   -- 
% - Impacts of voice credits --> 

\subsection{Does QV align better to true preferences?}
In our experiments, we show that QV aligns better to people's true preferences comparted to Likert surveys at least under the following assumption: First, the question of the survey focuses at making a decision among $K$ options. Second, there needs to be enough voice credits ($O(N^{3/2})$) for QV to perform well. Third, the options on the survey can be of different \textit{aspects} of the same subject matter or differet \textit{options} for the same subject matter. Fourth, QV seems to transfer well across domains. Both experiments showed significant effect size that QV is capable at assisting preferences among a crowd of people. 

The inherent difference between QV and Likert survey lies in the notion of cost when expressing opinions. Likert does not carry such costs. Any individual can elect any options, even meaning selecting ``very important'' for all of the options. One cannot do this in QV, QV participants were forced to give a value to each of the options to represent how much they approve or disaprove that option. In addition, all these values comes with a quadratic amount of cost. This cost might not be obvious to participant when they make the vote, but it confines participant's behaviors. In fact, we believe that the reason behind why QV shows better alignment to true preferences aligned with the concept ``Scarcity Frames Value'', a work by Shah et al. \cite{Shah2015a}. In this work, the authors showed that lower-income participants are less susceptible to context effects when compared with higher-income participants. They attributed this phenonomon that lower-income participants are better at trade-off thinking, which under resource constraint conditions, individuals focuses on what is needed and become more rational at decision making. We observe qualitative supports for this claim from experiment one, specifically when there is a change in voice credits. One participant (\texttt{P24194}), whose vote was decreased between the two surveys they recieved mentioned, ``I had fewer credits, so each vote seemed more expensive.'' Voice credits translate the idea of ``scarceness'' and resource constraint into a survey. 

Another difference between Likert and QV is the boundaries on the survey. Likert surveys are bounded by an ordinal scale that the survey conductor designed while QV is not. This means that for QV participants, there are a lot more ways and degree of freedom they can express their fine-grain preferences.From the free-form response text we collected during the experiment, one participant (\texttt{P9b3ae}) explicitly mentioned ``[\textellipsis] I would answer otherwise, if there were other options, such as not much, or a little bit.'' This shows support that participants exist different levels of importance but were not able to express them expressively in the Likert surveys. Similar issues were not presented in the QV Group responses. One could argue that it is the design of Likert surveys that bounded the results, however, a five or seven-point Likert survey can be viewed as the de facto method at collecting user attitudes. It is usually little, if not no, discussion in research studies where it justifies the use of a five or seven-point Likert in the survey. In fact, one related work discussed the use of different points in Likert surveys dated back to 1965 \cite{komorita1965number}. This research showed that if the options are homogeneous, a two-point Likert scale yields as high-reliability coefficient as a multi-category system. The research also emphasized that reliability should not be the only metric when deciding which scale is better than the other scale, much more the opposite. It implies that a scale should be chosen for the purpose it aims to serve and the context it is used in. Similarly, it is important to emphasize that our claim is not to veto the use of Likert surveys, but to pose an alternative method, QV, that aligns much closer to participants true preference and provides much more information when making a collective decision in a 1 in $K$ setting.



% - open questions on QV
%    - a better way to analyze likert, ordinal vs scalar. we had this issue, we are aware. we tried to introduce bahviors to mitage/minimize this impact. Doees 5/7 pt likert make a difference
%    - knapsack voting, rank based voting: allocating a budget, but not quadratic, it's inteeresting fo how it compares, rank-based voting shows reltaive preference 
%    - does the number of options matter. Is there a upper limit to voice credits (upper bound)
%    - we scoped for question 1 in K, are there questions that QV does not work. what we looked in. what what we did not look in. 
%    - interface problem, interface design
%    - continuative/carryover voice credits. <-- not so related. 
%    - mental model

% - wider implications
%    - summary of prior sections of disccusions
%    - but we are not saying likert is bad
%    - researchers consider cost at educatiing people on it


% limitations:
% > We cannot control all participants online but we designed incentative compatiable and implemented many attetion checks  / self report
% > likert mapping is limitation





\subsection{Where are QV's limitations}
Experiment two shows a slightly different and more nuanced result.
It is important to know that
the characteristics of the ballot options 
are different compared to experiment one.
On this ballot, the options are \textit{multiple aspects}
of a same question.
In other words, 
one requires all options 
to form the result of the final outcome.
Let us use an analogy:
If experiment one asks you
``how much do you prefer between
coke, fries, and burgers'',
the second experiment asks you
``how important do you think
freshness, taste, and texture 
of your beef patty are?''
Notice the options in the first question 
are independent while 
those in the second question 
collectively decide how good a beef patty is.

From an intensity perspective 
of the aggregated opinions, 
qualitatively we can observe that 
the preference result from QV aligns closer 
to the true preference than that from Likert. 
We have yet to examine its statistical significance 
due to an experiment design limitation. 
From a preference ranking perspective, 
neither of the results from the Likert survey 
and QV survey
diverged significantly 
from the incentive-compatible behaviors 
that the buyback group demonstrated. 
However, part of the Likert results 
did diverge significantly from the QV results, 
since aspects in Likert results clustered 
in the higher rankings, 
while aspects in the QV results 
spread out more across the lower rankings. 
Such finding echoes part of the conclusion 
in the first experiment, 
where QV is able to show finer-grained preference results 
than Likert.

In conclusion,
we can conclude that QV with a sufficient budget outperforms Likert surveys when the survey aims to elicit preferences in a 1 in $K$ setting.
There is not enough statistical evidence yet to conclude whether QV aligns to true preference better than Likert if the survey options are multiple aspects of the same subject. Nevertheless, both experiments demonstrated that QV elicits finer grain preferences from people than Likert in both cases, which is an advantage survey designer could leverage to gain more in-depth understanding of people's opinions.

\subsection{Impact of QV voice credits}
In experiment one, we also confirmed our hypothesis that the number of voice credits does impact the results. 
In fact, given a fixed set of voice credits, there exists is a finite set of ways one could allocate their votes. 
Different from Likert surveys, the votes change according to the number of options present.
Yet, the performance of QV does not come through before reaching an excessive amount of voice credits, as we see that QV with 36 voice credits unperformed QV with 108 and 324 voice credits.

From the change in participant's responses as the number of voice credits changed, we found supporting evidence that participants require enough votes to demonstrate their preferences.
For example, some participants that had a drastic increase of voice credits, from 36 to 324 voice credits, expressed devoting some options that originally had zero votes. 
\texttt{P2d9da} stated, ``Because now that I have a lot more credits, I felt that I could vote on more issues that mean something to me.'' The participant initially only voted for Environment; however, with 324 votes, the participants voted for all but Faith and Spiritual. This supports our quantitative finding that a limited amount of voice credits suppressed the performance of QV. Participants also reported being freer and submitted more fine-grain opinions. As one participant (\texttt{P54f23}) responded: ``The greater voice quantity allowed me to vary the differences in choices'' more and similarly \texttt{Pcc4aa} reported, ``with more credits i can show what i really like.''
This reflects that additional credits pushed participants to express more fine-grain preferences.
% If the number of voice credits increased, as expected, some participants uniformly increased the number of votes across all nine options, stating that they try to be fair. Also, logically, other participants would devote the additional voice credits to the items of their likes or dislikes. 

On the contrary, participants are forced to downsize their preferences if credits decreased. Many participants voiced their need to make trade-offs. \texttt{P9e5e6} said, ``I think I covered the bare basics.'' and \texttt{Pe37f2} said, ``Less to go around, so had to knuckle down and allocate the most to what I think is most important.'' 
Again, this means that it is crucial to have enough points if we want to reflect participant's preferences and they're intensity accurately.
The question of how to identify \textit{what} number of voice credits to use is still unknown.

\subsection{Design Implications}
Given these discussions, 
we propose the following design implications. 
First, QV provides more fine-grain results,
including the preference and the level of intensity,
a group has on a particular topic,
when deciding among one in $K$ options.
In the CSCW community,
we believe this can be applied to 
many collective decision-making processes.
Form electing a great project among many,
to redistributing limited resources among those in need,
QV serves as an alternative tool 
than traditional Likert surveys.
