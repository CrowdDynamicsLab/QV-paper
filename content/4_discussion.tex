\section{Discussion and Future Work} \label{discussion}
In this section, we discuss implications from the results of our two experiments. In addition, we lay out intriguing open questions stemmed from this study for future exploration.

\subsection{Does QV Align Better with True Preferences?}
In both RQ1 and RQ2, we ask about how well Likert and QV survey results align with people's incentive-compatible behaviors, one in the realm of public policy and the other in HCI research. From the two experiments, we showed that QV aligned significantly better to people's true preferences than Likert surveys in these two areas under the following conditions: (1) the question in the survey focused on making decisions based on the relative preferences among $K$ options, (2) the $K$ options were either different \textit{aspects} that jointly contributed to the same subject matter or paralleled \textit{options} for the same subject matter, and (3) the QV survey must provide sufficient voice credits ($\geq O(K^{3/2})$), where $K$ is the number of options) to participants. Both experiments showed a significant medium to high effect size for the difference between the degree of alignment in the QV groups and the Likert group. The results from the experiments suggested that QV had the potential to elicit more truthful preferences in collective decision making. We now discuss two potential reasons that may explain the result for RQ1 and RQ2.

\subsubsection{Costs and scarcity}
One explanation for QV's better alignment in the above conditions may be the inherent difference in the notion of costs between QV and Likert when individuals express their opinions. Expressing opinions in Likert does not carry any cost -- an individual could freely select any value, including extreme values, for all $K$ options if one prefer. In QV, participants must pay for their votes for each option at a quadratic cost under a limited budget. 

When costs and a limited budget both exist, participants are facing \textit{scarcity} of resources. In behavioral economics, \textcite{Shah2015a} concluded in their work, ``Scarcity Frames Value'', that individuals would focus on what is really needed and become more rational in decision making under resource constraints. They found that lower-income participants were less susceptible to framing effects than higher-income participants likely because lower-income participants faced stricter resource constraints and thus were better at trade-off thinking. 

In conditions where a survey focuses on understanding how people make trade-offs among $K$ options, as in our two experiments, the concept of scarcity built into the mechanism of QV may help participants make more rational decisions. We observed qualitative supports for this claim from experiment one. One participant (\texttt{P24194}), after experiencing a drop in the number of voice credits in the second QV survey, they commented, ``I had fewer credits, so each vote seemed more expensive.'' This comment suggested that participants experienced the idea of ``scarceness'' and resource constraint via the limited voice credit budget during the QV survey.

\subsubsection{Flexibility of expression}
Another potential reason why QV aligned better with incentive-compatible behaviors is a higher degree of flexibility in expressing opinions in QV (with sufficient voice credits) compared to Likert. A five-point Likert survey provides only five choices for each question, which may limit the way a participant voice their opinion. One Likert group participant in the first experiment (\texttt{P9b3ae}) explicitly mentioned ``[\textellipsis] I would answer otherwise, if there were other options, such as not much, or a little bit.'' Participants wanted to express more fine-grained attitudes while a five-point Likert survey forced them to map their feelings onto a limited fixed scale. QV, in contrast, allows participants to specify the relative distance between two options with greater flexibility, as long as the total cost does not exceed the given credits. The flexibility may enable participants to stay closer to their incentive-compatible preferences and do not need to map to an arbitrary scale.

We compared QV against a five-point Likert scale in this study because the five-point Likert scale is one of the most commonly used Likert scales in various fields \cite{malhotra2006basic}, including public policy and HCI. One may conjecture that a seven-point or eleven-point Likert scale may allow more flexibility than a five-point Likert scale. Debates about the scale format in Likert surveys started in as early as 1965 \cite{komorita1965number}. Some studies found little differences among different scale formats while others did find certain differences \cite{dawes2008data}. We leave the comparison of Likert surveys in other scale formats and QV as an open question to future research.
% This research showed that if the options are homogeneous, a two-point Likert scale yields as high-reliability coefficient as a multi-category system. The research also emphasized that reliability should not be the only metric when deciding which scale is better than the other scale, much more the opposite. It implies that a scale should be chosen for the purpose it aims to serve and the context it is used in. However, in modern research that utilize Likert surveys, there was often little to no discussions describing why the researchers used a specific type of Likert survey. 

\subsection{Effects of the Amount of Voice Credits}
In RQ3, we are interested in how the number of voice credits available to participants impact the QV survey results empirically. In experiment one, quantitative analysis showed that QV did not elicit more truthful preferences until reaching a sufficient amount of voice credits, as we saw that QV with 36 voice credits ($O(K)$) under-performed QV with 108 ($O(K^{3/2})$) and 324 ($O(K^{2})$) voice credits. We found potential explanations through participants' qualitative responses. 

When participants had only 36 credits in QV, some of them voiced their need to make hard trade-offs. \texttt{P9e5e6} said, ``I think I covered the bare basics.'' and \texttt{Pe37f2} said, ``Less to go around, so had to knuckle down and allocate the most to what I think is most important.'' These responses indicated limited flexibility when the participant's tried expressing their preferences with a small budget.

On the other hand, when given increased voice credits (108 or 324 credits) in their second QV survey, some participants expressed their appreciation for an increased degree of freedom to express their opinions. \texttt{Pcc4aa} reported, ``with more credits I can show what I really like.'' and \texttt{P2d9da} stated, ``Because now that I have a lot more credits, I felt that I could vote on more issues that mean something to me.'' The different qualitative responses for QV36, QV108 and QV324 may not only explain why QV with more voice credits performed better than QV36 in the first experiment, but also lend support to higher level of flexibility being a potential reason for why QV outperformed Likert in the degree of alignment. 

While having fewer voice credits may worsen QV's degree of alignment with participants' incentive-compatible preferences, QV with a stringent budget may be better at eliciting the options participants care the most since it encourages harder trade-offs, based on the qualitative responses above. Future research could explore this potential effect of QV more closely.

In the first experiment, we explored up to a budget of $O(K^{2})$ voice credits, where $K$ is the number of options in the survey, specifically 324 credits. While QV aligned better than Likert up to this amount of voice credits and the percentage of credits used remained high (with a median of 98\%), we suspect that too many voice credits may create excessive cognitive loads to participants. Where the upper bound is for the amount of voice credits empirically remains an open question for future work.


\subsection{Open Questions and Future Work}
QV is a relatively new area of research. Comparing the alignment of Likert and QV with user's true preferences is challenging. During the study, we identified various open questions that were yet to address. In this subsection, we propose them as future research directions.

\subsubsection{Comparing ordinal data With numerical data}
To compare ordinal Likert data with numerical donation amounts or set prices, we mapped the five-point Likert scale to integers in the range of $[-2, 2]$. We used the number of votes in QV directly since they are numerical. We made such a decision because selecting "Neutral" (mapped value = 0) in Likert had a similar meaning as casting zero vote in QV.

Whether our approach of mapping Likert data to metric values is the best approach to compare ordinal data with numerical data is debatable. At the same time, identifying the best measure to do so is challenging -- the best way to analyze ordinal Likert data remains a debate till this date \cite{gob2007ordinal}. Future research could explore if there are alternatives to perform such a comparison that circumvents the challenge of mapping ordinal data. 

\subsubsection{Comparing QV with other surveying methods}
% In the previous subsection, we mentioned that Likert survey choices could limit individuals' degree of freedom to express their attitudes. One open question includes investigating if another type of Likert survey influences participants' behavior compared to QV. In addition,
Despite Likert survey being one of the most-used surveying techniques, we are curious about how other voting mechanisms that involve the concept of resource constraint or relative-preference elicitation perform compared to QV. Examples include but are not limited to knapsack voting \cite{goel2015knapsack} and ranked-base voting \cite{ledo2018evaluation}. Knapsack voting also uses a budget, but the cost of QV's vote grows quadratically while that of knapsack voting increases linearly. Ranked-based voting focused on eliciting relative rankings among options, which incurs trade-off thinking, but does not show the magnitude of differences between the options, unlike QV. The coexistence of differences and similarities of these voting mechanisms with QV makes the comparison of their performances an interesting open question.

\subsubsection{Upper bound of the number of options}

In our two experiments, the first had 9 options and the second had 5 options on the survey. In both cases, QV performed well, suggesting that participants could make effective trade-offs across up to 9 options. However, our study did not identify the upper bound of the number of options users can handle comfortably on a QV survey. One can imagine the difficulty for QV participants to vote among dozens of survey options. In fact, a work by Iyengar et al. \cite{iyengar2000choice} observed that more choices may not necessarily increase participants' satisfaction, suggesting that people were not good at making choices across an extensive array of options. Therefore, the same question applies in our case -- is there a limit of how many options could be on a QV survey to maintain high-quality data collection?

\subsubsection{Generalizability to different types of survey}
In this study, we examined QV in two settings. We chose settings that made sense to translate into QV surveys and leveraged prior research. We did not exhaustively examine the type of survey questions that work with QV and those that may not. Hence, readers should take caution in generalizing our results to other types of survey.

In the first experiment, the survey asked participants to choose among $K$ paralleled options for the same subject matter. In the second one, the survey focused on choosing among $K$ aspects that jointly contribute to the same subject matter. The purpose in both surveys were related to relative preferences and trade-offs. We do not yet know if surveys options with other types of relationship would work for QV, e.g., surveys that consist of independent options that do not connect with one another.

Similarly, our study only tested QV in the field of public policy and HCI research. Many other disciplines made use of Likert surveys to make collective decisions. Future research can explore if QV elicits more truthful preferences than Likert in other fields.

\subsubsection{Interface design and mental models}
The final open question is designing a simple, intuitive QV interface for empirical use. QV involves more complicated calculations than Likert. A well-designed interface should reduce a user's cognitive load to help them make accurate decisions easily. Currently, after our iterative-design process, we provided participants information such as the number of votes per option, the voice credits used and remained, and how they allocate the voice credits to each option. 

Different interface designs could nudge users to behave in one specific way instead of another. How the interface should provide voters with these information in an optimal way remains an open question. Understanding individuals' mental models of during voting in QV may inform better designs. Finally, we need to investigate the QV interface design for mobile and tablet devices.
% Besides, showing an individual how they had allocated their votes could also potentially interfere and encourage voters to vote for or against an option

\subsection{When to Use QV?}
We showed promising results of QV in our study -- QV elicited more truthful preferences from participants than a five-point Likert scale when making a collective decision to choose among $K$ options. Given the popularity of Likert survey in a wide range of disciplines that involve self-reporting, our research asks an intriguing question of whether there is an alternative survey method that can elicit accurate opinions. However, it is critical to reaffirm that the goal of this study is not to claim one survey method should replace another. As aforementioned, there still exist many open questions to understand the nuances in QV. Survey creators should carefully consider the pros and cons (e.g., higher cost to educate participants) of QV and select the best suiting survey method in their contexts.




% \subsection{Where are QV's limitations}
% Experiment two shows a slightly different and more nuanced result.
% It is important to know that
% the characteristics of the ballot options 
% are different compared to experiment one.
% On this ballot, the options are \textit{multiple aspects}
% of a same question.
% In other words, 
% one requires all options 
% to form the result of the final outcome.
% Let us use an analogy:
% If experiment one asks you
% ``how much do you prefer between
% coke, fries, and burgers'',
% the second experiment asks you
% ``how important do you think
% freshness, taste, and texture 
% of your beef patty are?''
% Notice the options in the first question 
% are independent while 
% those in the second question 
% collectively decide how good a beef patty is.

% From an intensity perspective 
% of the aggregated opinions, 
% qualitatively we can observe that 
% the preference result from QV aligns closer 
% to the true preference than that from Likert. 
% We have yet to examine its statistical significance 
% due to an experiment design limitation. 
% From a preference ranking perspective, 
% neither of the results from the Likert survey 
% and QV survey
% diverged significantly 
% from the incentive-compatible behaviors 
% that the buyback group demonstrated. 
% However, part of the Likert results 
% did diverge significantly from the QV results, 
% since aspects in Likert results clustered 
% in the higher rankings, 
% while aspects in the QV results 
% spread out more across the lower rankings. 
% Such finding echoes part of the conclusion 
% in the first experiment, 
% where QV is able to show finer-grained preference results 
% than Likert.

% In conclusion,
% we can conclude that QV with a sufficient budget outperforms Likert surveys when the survey aims to elicit preferences in a 1 in $K$ setting.
% There is not enough statistical evidence yet to conclude whether QV aligns to true preference better than Likert if the survey options are multiple aspects of the same subject. Nevertheless, both experiments demonstrated that QV elicits finer grain preferences from people than Likert in both cases, which is an advantage survey designer could leverage to gain more in-depth understanding of people's opinions.



% \subsection{Design Implications}
% Given these discussions, 
% we propose the following design implications. 
% First, QV provides more fine-grain results,
% including the preference and the level of intensity,
% a group has on a particular topic,
% when deciding among one in $K$ options.
% In the CSCW community,
% we believe this can be applied to 
% many collective decision-making processes.
% Form electing a great project among many,
% to redistributing limited resources among those in need,
% QV serves as an alternative tool 
% than traditional Likert surveys.
