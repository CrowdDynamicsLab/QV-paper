\section{Methods -- experiment Two: Importance of Video Elements}
\label{method-2}
The first experiment answered RQ1 and showed that QV aligned closer to the participants' true preferences compared to Likert scale when choosing among $K$ independent options of the same subject matter. To strengthen our results and examine the generalizability of QV, we designed a second experiment to answer our second research question: \begin{quote}RQ2: How well do QV responses and Likert-scale responses align with the respondent's true preferences in a survey where the survey respondent chooses among $K$ dependent options that jointly contribute to the same topic?\end{quote}  
Since relationships and interactions among ballot options may impact how users make trade-offs, we want to examine if the results from QV also align better with people's true preferences than does the Likert scale in RQ2.

We hypothesized that QV would outperform Likert in accurately eliciting participants' true preferences in the new setting. We changed the application domain from public policy to HCI {\change{, an important research area in the CSCW community,}} since HCI user studies often rely on eliciting preferences from users to inform designs that involve trade-offs and in turn create better user experiences. These characteristics made HCI studies an excellent domain to test out QV. To verify our hypothesis, we designed a within-subjects study that elicited participants' preferences on different video elements using QV, Likert, and a pricing task. 

In this section, we first explain how we selected video streaming experience as the HCI study scenario. Then, we demonstrate the experiment workflow accompanied by the interfaces of the experiment. Finally, we explained the analysis approach.

\subsection{Choice of HCI study} \label{exp2-hci}
We set out to find a typical HCI study where UX/UI researchers survey users to understand what features to prioritize. In the end, we decided to use the research scenario of understanding users' preferences among various video and audio elements under network or monetary constraints~\cite{molnar2013comedy, oeldorf2012bad}.

Research on video and audio elements of video playback from the lens of HCI is relatively mature. Understanding how users with bandwidth constraints made trade-offs across multiple videos and audio elements~\cite{molnar2013comedy, oeldorf2012bad} is a typical example for which of the $K$ dependent options of the same topic would people prioritize under constraints. \textcite{oeldorf2012bad}, for example, conducted a study to examine the differences in participants' perceptions between three video bit rates, three video frame rates, and two audio sampling rates across three types of video content via a 5-point Likert scale. 

We proposed a similar user research topic as in \textcite{oeldorf2012bad}'s study. We designed the experiment to answer the following question: ``Given a video with unsatisfying quality, under limited bandwidth, how should the bandwidth be allocated to enhance the five visual and audio elements, including motion smoothness~\cite{huynh2008temporal}, audio stability~\cite{hardman1998successful}, audio quality~\cite{knoche2008low}, video resolution~\cite{knoche2005can}, and audio-video synchronization~\cite{steinmetz1996human}, to obtain an acceptable video streaming experience from the viewers' perspective?'' We selected the five video elements based on prior work. For their detailed definitions, please refer to \Cref{elem_def}. To our knowledge, no prior work has studies the combination of five elements in a single experiment.

Prior work suggested that the type of video affected users' perceptions for video elements. In this experiment, we used a 90-second weather forecast video for the United States. We chose a weather forecast video for two reasons. First, the concept of a weather forecast is generic and universal. The terms used in the weather forecast are usually easy to understand. Second, since we are studying both audio and visual elements, we wanted a video that conveyed information via both visuals and speech. In a weather forecast video, the meteorologist usually spoke aloud the weather while pointing at visual cues such as icons and numbers.

In the next section, we describe how we conducted the video elements trade-off experiment to compare QV and Likert scale's ability in truthfully reflecting users' preferences across the video elements.

\subsection{Experimental Flow}
We recruited participants on Mechanical Turk through the CloudResearch platform~\cite{litman2017turkprime}. Like our first experiment, we used a pre-survey to match the participants' distribution with the U.S. population in age and education based on 2018 US census estimates. The average completion time was 35 minutes 42 seconds and all participants received \$6 as base pay and a bonus up to \$2. All participants followed six steps. The six steps were (1) demographic survey, (2) tutorials and attention checks, (3) a video playground, (4) Likert and QV surveys, {\change{(5) a filler task}}, and (6) a design task. Now we explain the six steps in detail. {\change{We include the experiment flow diagram in the appendix and present the experiment protocol as supplementary materials.}}

\textbf{Step 1. Demographic Survey} We greeted participants with a consent form. In the consent form, we presented the goal of the study as understanding how people think about the importance of the different elements during video streaming. We did not reveal to the participants that this experiment aimed to compare Likert and QV until they completed the survey. Once participants gave their consent, they would fill out a demographic survey that contained questions identical to those in the first experiment.

\textbf{Step 2. Tutorials and Attention Checks} In step two, we provided two tutorials to the participants. All participants would first read through a tutorial that defined the five video elements used in the experiment, via textual explanation and pairs of video examples. On the next page, they needed to answer five multiple-choice questions about the definitions of video elements and two attention checks designed to see if audio and video played fine on the participant's device. Participants qualified for continuing the experiment only if they answered fewer than two questions incorrectly. This step made sure participants fully understood the terminologies used in the rest of the experiment.

Participants then moved on to a tutorial on how QV works, with a short instructional video supplemented with text. They had a chance to play with a QV interface similar to that in \Cref{fig:qv_donation}. Once the participants were confident that they understood QV, they needed to complete the same QV quiz in experiment one. The system disqualified a participant immediately if they answered two or more questions wrong.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.8\textwidth, keepaspectratio=true]{content/image/video_playground.png}
    \caption{
        The real-time video element interface allows participants to adjust video playback elements and understand how differences in the elements impact their viewing experience. We selected four levels of quality settings for each element according to prior research, ranging from an unacceptable quality to a good quality. The technical details of this implementation are described in ~\cref{appx_video_interface}.
    }
    \label{fig:exp2_playground}
\end{figure}

\textbf{Step 3. Video Playground} To increase this experiment's fidelity, we framed the study as a market research initiative by a {\change{fictitious}} company (participants were not aware that the company was {\change{fictitious}}), with a goal to provide a video-streaming product in cars using satellite-based Internet. We first showed the participants the ``current prototype" of the company's streaming service, which simulated what a weather forecast video played under limited bandwidth would look like, with all five elements at the worst quality in the range we designed to study\footnote{Motion smoothness: 2.5 fps; Audio stability: 20\% packet loss rate; Video resolution: 120x90 at 32 kbit/s; Audio quality: 8kHz sampling rate; Audio-video synchronization: visuals play 2000ms ahead of the audio}. 

To help participants better understand the impact of various enhancement levels for each element on the current prototype, we led them to a video playground shown in \Cref{fig:exp2_playground}. This playground allowed participants to use the control panel to adjust the levels of enhancements for all five video elements and see real-time changes in the video on the top of the page, i.e., the video played in the most recent combination of quality levels. Participants could pause and play the video at any time, and replay the video as many times as they choose. We encouraged participants to test out different combinations freely in this playground to help them understand the impact of each element and how the five elements interact. We asked participants to describe how changing the elements impacted their experience in a free-form text question to make sure participants did experience different settings.

For each element, we provided a slider with four levels, Level 0 at the lowest quality and Level 4 at the highest. We designed the intermediate levels based on prior research such that the changes between each level of an element had a quasi-linear impact on viewers' perception. The four levels of the five video elements are listed below, from Level 0 to Level 3:
\begin{itemize}
    \item Motion Smoothness~\cite{huynh2008temporal}: 2.5 fps, 6.25 fps, 8 fps, and 25 fps
    \item Audio Stability~\cite{hardman1998successful}: 20\%, 10\%, 5\%, and 0\% probability in packet loss
    \item Video Resolution~\cite{knoche2005can}: 120x90 at 32 kbit/s, 168x126 at 64 kbit/s, 240x180 at 96 kbit/s, and 240x180 at 224 kbit/s; encoded in the WMV2 (Windows Media Video 8) codec 
    \item Audio Quality~\cite{knoche2008low, noll1993wideband}: 8kHz, 16kHz, 24kHz, 32kHz; encoded using the AAC (Advanced Audio Coding) codec
    \item Audio-Video Synchronization~\cite{steinmetz1996human}: 2000ms, 1250ms, 750ms, 0ms; visual ahead of audio
\end{itemize}

\textbf{Step 4. Surveying Preferences} After experiencing how different enhancements on the five video elements impacted their viewing experience, we collected participants' opinions on how critical the improvements on each video element in the current prototype were for them to understand the weather forecast video. Participants completed  a QV survey and a Likert scale survey in a randomized order to prevent ordering effect. Since this was a within-subjects study, we randomized the display order of the elements on the surveys to minimize carryover effect and ordering effect. The QV interface was similar to that in experiment one. We designed the credit budget to be 100 voice credits, the best option based on experiment one, i.e., $K^2 \times O$, where $K=5$ and $O=4$ in this case.

{\change{
\textbf{Step 5. Filler Task} Before moving on to the task for eliciting the participant's true preferences, we designed a survey as the filler task that asked participants' about their consumption preferences for subscription tiers across several well-known streaming services, telling them that the survey results would be an important part of the next task. This survey aimed to prevent participants from translating their survey responses to the video elements survey directly to the next task.}}

\textbf{Step 6. Design a Product} To capture a participant's true preferences on how much they \textit{value} each of the five video elements, we created an incentive-compatible product design task to elicit their true willingness-to-pay for the elements. To create a high fidelity scenario, we told participants that the system assigned them into the designer group, and their job was to design a streaming service for cars via satellite internet. They should design and price their product such that another participant from the buyer group ({\change{fictitious,}} but the participants were not aware), who we claimed to have matched for them based on demographic information and the consumption preference survey, would be willing to purchase the product. We emphasized that their product should be affordable and should allow the buyer to understand the weather forecast video. To incentivize the participants, we offered them 10\% of the final price they proposed as their bonus if the buyer decided to purchase their product.

We guided participants to complete the task in two {\change{steps. In}} the first step, participants needed to select one of the two qualities for each video element, a lower quality and a higher quality, and ``assemble'' the product. Quality one refers to level 0 in the playground, the worst one they had experienced. For quality two, we selected the quality levels in the playground that matched what prior research showed to be the ``acceptable level''\footnote{Motion smoothness: 6.25 fps; Audio stability: 10\% packet loss rate; Video resolution: 240x180 at 96 kbit/s; Audio quality: 16kHz sampling rate; Audio-video synchronization: visuals play 750ms ahead of the audio}. Participants saw real-time changes to the video as they updated the qualities. Participants were essentially making a binary decision of whether a video element was ``important'' or ``not important'' for them, and equivalently for the buyer that had similar preferences as them. 

In the second step, participants set a price they thought to be reasonable for each of the five elements between \$0 - \$4 based on the qualities they selected and how much they thought the buyer would value them. The sum of these prices was the total price of the product. We reminded participants throughout the task that the higher they priced the elements, the more their bonus could be. However, if they overpriced any element from the buyer's perspective, the buyer might not purchase their design and they would fail to gain any bonus. 

The goal of this design was to elicit the truthful willingness-to-pay for each participant as the their true preferences. Our set-up was incentive-compatible and the best strategy for the participants was to price based on how they themselves valued each of the elements. If they priced the product higher than their accurate valuations, the ``buyer'' with similar demographic and consumption preferences as them would reject their proposal, given that the buyer would likely value the elements the same way. Vice versa, if the participants set their prices to be lower than their actual willingness-to-pay, they would lose out on earning a higher bonus.

\subsection{System Design}
We reused the QV interface from experiment one in our second experiment. To create real-time adjustments in video qualities, we pre-generated video-only and audio-only files of different qualities. When a participant changed an audio or video quality setting, the system served the correct combination of video and audio files. We used JavaScript to adjust the video-audio synchronization in the front-end at real-time.

This design balanced the need for a high network speed to stream every configuration from the server and the need for a powerful client to compute the video and audio qualities. The experiment source code for experiment two is publicly available\footnote{https://github.com/a2975667/QV-buyback}. More details of the system implementation are provided in \Cref{appx_video_interface}.

\subsection{Analysis Method}

Since RQ2 is also about the degree of alignment, similar to RQ1, we followed a similar analysis approach as described in \Cref{method_exp1}. In experiment two, we compared the alignment between survey responses with the prices set in an incentive-compatible scenario. We used the same definition of ``alignment" and the same metric for alignment, cosine similarity angle, as explained in \Cref{alignment_metric}. For the Likert group, we mapped the ordinal responses into a vector where the result for each video element ranges from $-2$ to $2$. For QV, the vector contains the number of votes for each video element. Then, we computed the cosine similarity angle between a Likert or QV vector and the absolute prices set by the same participant,.

Once we obtained the two sets of cosine similarity angles, one for Likert and one for QV, we applied the same Bayesian formulation detailed in \cref{exp1:The Bayesian Model} to model the mean cosine similarity angle in both groups. Then, we compared the two distributions to see if they how far they are apart.