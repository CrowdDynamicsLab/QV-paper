\section{Introduction}

% Surveys are a common instrument to gauge opinion for scholars in the CSCW community, the social sciences, and many other research areas. The Likert ordinal scale~\cite{likert1932technique} is a familiar instrument in surveys that elicit ratings, where survey takers indicate their level of agreement, satisfaction, or perceived importance along an intensity scale \cite{moors2016two}. Nearly a hundred years after~\textcite{likert1932technique}, we find the ordinal scale in widespread use in online surveys, including those on Qualtrics, SurveyMonkey, and Google Forms platforms. Two observations prompt this study. First, the way researchers elicit preferences from survey-takers through the online incarnation of these ordinal scales is virtually indistinguishable from the Likert scale surveys administered on paper-based forms nearly a century ago. \tc{We wonder if the ubiquity of computational devices can popularize survey tools that were accurate but hard to access on paper.} Second, one of the great advantages of online surveys is that scholars can potentially reach a large audience. However, prior work indicates that self-reports by survey-takers, including responses recorded via an ordinal scale, can be inaccurate~\cite{araujo2017much, vavreck2007exaggerated} and deviate from the participant's \textit{true preference}, i.e., a preference based entirely on their honest opinions and own will. In this paper, we ask: \textit{Do the affordances of modern computer interfaces, coupled with a modern computer's ability to make real-time calculations, allow us to develop novel, screen-based techniques that elicit more truthful responses from survey-takers?}

Surveys are a common instrument to gauge opinion for scholars in the CSCW community, the social sciences, and many other research areas. The Likert ordinal scale~\cite{likert1932technique} is a familiar instrument in surveys that elicit ratings, where survey takers indicate their level of agreement, satisfaction, or perceived importance along an intensity scale \cite{moors2016two}. Nearly a hundred years after~\textcite{likert1932technique}, we find the ordinal scale in widespread use in online surveys, including those on Qualtrics, SurveyMonkey, and Google Forms platforms. Prior work indicates that self-reports by survey-takers, including responses recorded via an ordinal scale, can be inaccurate~\cite{araujo2017much, vavreck2007exaggerated} and deviate from the participant's \textit{true preference}, i.e., a preference based entirely on their honest opinions and own will. While scholars utilize online surveys to realize the ability to reach a large audience, the issue of inaccurate self-reports remains because the way researchers elicit preferences from survey-takers through the online incarnation of these ordinal scales is virtually indistinguishable from the Likert scale surveys administered on paper-based forms nearly a century ago. Therefore, in this paper, we ask: \textit{Do the affordances of modern computer interfaces, coupled with a modern computer's ability to make real-time calculations, allow us to develop novel, screen-based techniques that are hard to use on static paper to elicit more truthful responses from survey-takers?}

We examine our motivating question within the context of surveys where the survey creator made decisions by aggregating respondents' elicited opinions. At the same time, these decisions could potentially impact these survey respondents. Examples of such surveys include those that ask respondents about which policies the government ought to pursue~\cite{pew_spending}, ask users for their opinions on different interface designs~\cite{ledo2018evaluation}, and ask customers about what products to stock at grocery stores~\cite{nielsen}. The decision-maker (i.e., the group or the organization that uses the survey results) then uses the survey aggregated responses to implement one of the several options presented to the survey-takers. Often, these decision-makers have limited resources (e.g., money, time, space, manpower) and cannot realize all the options presented to the survey-takers. In this paper, we term such surveys, where respondents help the decision-maker choose among a set of options under limited resources, \textit{resource-constrained surveys}. More specifically, we focus on resource-constrained surveys where respondents have some utility in the survey outcome (e.g., the specific policy that the government adopts), even though the exact value of the utility or the time to receive that utility may remain uncertain. In contrast, surveys that, for example, aim to develop rankings by asking respondents to identify their favorite hobby, place to visit, or their favorite celebrity, are not survey types that we consider in this paper. 

In resource-constrained surveys, decision-makers ask the survey respondents two questions: \textit{how much} do you prefer each option; how would you \textit{prioritize} (i.e., rank) the options? While in the past century, decision-makers have used ratings and rank-based survey instruments to answer such questions, the instruments answered \emph{one} of the two questions, but not both~\cite{moors2016two}.  Next, we examine two common survey techniques, one based on ratings and the other based on rankings.


% Decision-makers often utilize surveys to aggregate opinions from the crowd to inform decisions. Understanding a crowd's preferences and priorities across a set of options under resource constraints is a common use case of surveys. \tc{When there are insufficient resources, such as money, time, space, and workforce, it is impossible to satisfy all the needs, creating a resource constraint scenario. Examples of these scenarios include public polling surveys designed to allocate government budgets \cite{pew_spending}, surveys to assess designs in human-computer interaction (HCI) interfaces \cite{ledo2018evaluation}, and surveys to prioritize what products to stock in grocery stores \cite{nielsen}.}



% The Likert ordinal-scale is the current norm survey settings~\cite{moors2016two}, including resource-constrained  decision-making, because it is easy to administer and understand. For example, the Pew Research Center administers annual surveys with a 3-point Likert-like scale to \tc{know} how participants would budget federal government resources~\cite{pew_spending}. In a Likert-like scale survey, participants can freely select ``increase spending'' for all government program areas without considering its feasibility under a limited government budget, a behavior known as the ``extreme responding'' bias \cite{batchelor2016extreme, furnham1986response, meisenberg2008acquiescent}. In this case, the inability to accurately elicit people's true preferences can lead to sub-optimal government budget allocation. 



% \tc{When making decisions under resource-constrained scenarios}, decision-makers ask the crowd two questions: (a) \textit{how much} do you prefer each option, and (b) how would you prioritize (i.e., rank) the options? Throughout the past century, researchers developed various forms of surveys to elicit true preferences via self-reporting. Ratings and rankings approaches are two of the primary categories in survey methods; they focus on answering one of the two questions above, but not both \cite{moors2016two}. 

% Ratings approaches, such as those used in Likert scales \cite{likert1932technique}, elicit attitude intensity to answer question (a), while ranking approaches that use a ranking scale \cite{starch1923methods}, ask question (b) \cite{moors2016two}. These limitations motivated us to examine an alternative that combines approaches from ratings and rankings, Quadratic Voting (QV), a relatively new voting mechanism proposed by \textcite{posner2018radical}. In this paper, we empirically tested how well QV elicits true preferences in resource-constrained collective decision-making processes compared to the Likert scale, the current norm in these processes \cite{moors2016two}.

% In order to find truthful answers, researchers have developed various forms of surveys to elicit \textit{truthful} self-reported preferences \cite{stone1999science}. 
% Rating and ranking-based surveys are the two major categories \cite{moors2016two}, each with their strengths and weaknesses. 

In a rating-based survey, survey takers indicate their level of agreement, satisfaction, or perceived importance along an intensity scale~\cite{moors2016two}. The Likert scale is the most commonly used rating-based technique that elicits the \textit{strength} of a survey taker's preferences~\cite{likert1932technique}. Rating-based surveys are easy to understand and administer; this may explain why the Likert scale is the current norm in many survey settings. The Likert ordinal scale is problematic in the resource-constrained survey types that are of interest in this paper. Consider, for example, the Pew Research Center annual surveys with a 3-point Likert-like scale that ask how participants would budget federal government resources~\cite{pew_spending}. Respondents in such a survey can freely choose  ``increase spending'' on the scale for \textit{all} government program areas. However, a government with limited resources cannot operationalize this option. This behavior is known as ``extreme responding'' bias~\cite{batchelor2016extreme, furnham1986response, meisenberg2008acquiescent}. A Likert scale survey does not safeguard against exaggerated or unrealistic preferences~\cite{araujo2017much, vavreck2007exaggerated}. Furthermore, it may inaccurately elicit how participants \textit{prioritize} \tc{the options since each option is independently assessed~\cite{alwin1985measurement}}. 

Ranking-based surveys focus on understanding how participants \textit{prioritize} among the possible options~\cite{moors2016two}. For example, participatory budgeting~\cite{cabannes2004participatory,goel2015knapsack,Goel2016, Lee2014, benade2020preference} is a promising alternative for governments to ask citizens to prioritize resource allocation.  In participatory budgeting, voters are asked to identify a subset $I$ among projects $P$ on which they would like the government to allocate, and each project $i \in P$ has a fixed implementation cost $c_i$ known to the survey taker. The survey-taker is also given the total budget available $B$ to the decision-maker (e.g., local city council) and each respondent is asked to pick a subset $I \in P$ of projects such that the total costs $\sum_{i \in I} c_i \leqslant B$ stays within the budget. There are two main challenges with ranking-based surveys. First, ranking-based surveys cannot elicit the \textit{strength} of a participant's preferences, i.e., the degree to which participant prefers option A over B. Second, ranking surveys that use participatory budgeting may be less easy to adapt to CSCW or social science surveys, since the decision-maker, who creates the survey, has to assign implementation costs to each option and an overall budget. For example, in a survey about interface design, the decision-maker would need to assign costs to each interface element on which they are eliciting an opinion. Furthermore, unlike the typical participatory budgeting scenario, where project costs involve allocating \textit{the respondent's tax dollars,} in typical social science and HCI surveys, the implementation of an option involves no obvious cost to a participant. 

% Besides, performing statistical analysis on rankings data is challenging \cite{alwin1985measurement}.


% a resource-constrained collective decision-making scenario where citizens vote for the projects that go into a government budget, participants use ranked voting to order the list of projects according to their relative preferences \cite{benade2020preference}. 

In this paper, we examine Quadratic Voting (QV) as a possible preference elicitation mechanism in online surveys. \textcite{posner2018radical} recently advocated for the use of QV as an alternative for the traditional one-person-one-vote system. In the \textcite{posner2018radical} formulation, each person has access to a fixed number of voice credits $B$ (i.e., similar to the budget in participatory budgeting) to allocate across options. Then, each person can cast more than one vote (for, or against) for each option, with the proviso that the votes have a quadratic cost. Thus, casting $n_k$ votes on option $k$ would cost $c(n_k) \propto n_k^2$ in voice credits. Furthermore, the total cost in voice credits across all options cannot exceed the budget $B$. Therefore, a person casting positive or negative votes across $k$ options has to satisfy $\sum_k n_k^2 \leqslant B$, where $n_k$ is the number of votes cast on option $k$. 

% At the outset, it is unclear how well QV will translate to the kinds of surveys that we examine in this paper.

What makes QV of interest to the CSCW community that uses online surveying tools is that it appears to straddle the two familiar elicitation mechanisms of rating and ranking. While respondents can cast any number of voice credits modulo budgeting constraints on an option and thus indicating the strength of a preference, unlike Likert, they \textit{cannot} allocate a high number of votes to \textit{all} options. Notice that the quadratic vote costs imply that every additional vote on an option, say a change from $n$ to $n+1$ votes, increases the marginal cost by $(n+1)^2-n^2=2n+1$ voice credits. Due to the quadratic budget constraint, QV gently nudges the respondents to prioritize among the options. Prior work further showed that aggregated QV results of an option in a sample approximate a normal distribution in contrast to  the ``extreme responding'' behavior found in Likert surveys~\cite{quarfoot2017quadratic}.~\textcite{Lalley2018} also proved \textit{robust optimality} properties of QV, suggesting that QV may be incentive-compatible (i.e., truth-telling is the dominant strategy for the respondent). Yet, to our best knowledge, this optimality assertion lacks empirical validation in the survey contexts of interest to this paper. Since QV requires real-time calculations to let the respondent know if they are allocating voice credits across options within the overall budget, QV requires a computational engine. The ubiquity of smartphones and tablets, the desire for the CSCW community to reach a wide audience through online surveys, and QV's robust optimality all suggest that QV is worth investigating as an alternative online survey instrument. \tc{Though there were many unanswered questions -- from designing QV interfaces to communicating what QV is to users -- we believe examining whether QV elicits more accurate responses from participants than Likert scale surveys is an important first step.}



% However, that QV requires a computational engine is also problematic for scholars who wish to reach respondents through both online and paper-based surveys.
% While QV is an thus an attractive option to test given the ubiquity smartphones and a
% Challenges with eliciting accurate self-reported preferences in resource-constrained decisions motivated us to examine an alternative, Quadratic Voting (QV), a relatively new voting mechanism proposed by \textcite{posner2018radical}. In this paper, we empirically tested how well QV elicits true preferences in resource-constrained collective decision-making processes compared to the Likert scale. \lwt{Here, we define an individual's ``true preference'' as a preference based entirely on their honest opinions and their own will.} \tc{We measure it through the outcome of an incentive-compatible task in our study.}
% There had been long debates on the pros and cons of rankings and ratings approaches. Nonetheless, when researchers developed many of these survey methodologies, the mediums of conducting surveys were paper and landlines. The current generation of survey tools, such as Survey Monkey and Qualtrics, simply translated the traditional approaches from paper to online. Given the ubiquity of personal computing devices today, such as \tc{smartphones, tablets,} and laptops, we ask: is there a computational-powered survey methodology that elicits both the rankings \textit{and} ratings of a set of options accurately in a resource-constrained collective decision-making process? In this study, we investigate Quadratic Voting (QV) \cite{posner2018radical} as a potential response to this question. 

% Participants rank and rate the potential options in QV at the same time. In QV, expressing an opinion is not free -- each participant receives a fixed budget of voice credits, i.e., the ``currency" in QV, and pays a quadratic cost for the votes they cast. More formally, casting $n$ votes for or against an option costs $n^2$ voice credits. Participants vary the number of votes to represent the strength of their preference, i.e., they \textit{rate} an option with the number of votes. But as the number of votes on an option increases, the marginal cost for each extra vote increases as well; in other words, each additional vote becomes more expensive. Participants may cast as many votes as they choose on any option, as long as the total cost for the votes does not exceed the fixed given budget of voice credits. The limited budget and increasing marginal cost \tc{encourage} participants to make trade-offs among options, equivalent to \textit{ranking} them. Prior work further demonstrated that QV outcomes approximated a normal distribution\tc{ instead of }the polarized responses from Likert scale survey results on highly-debated topics \cite{quarfoot2017quadratic}. However, to the best of our knowledge, no one has empirically explored whether QV \tc{can elicit} more accurate self-reported preferences.
%\textcite{lalley2018quadratic} proved that the QV mechanism is more Pareto-optimal than many traditional voting mechanisms.

In our study, we examined QV's ability to elicit true preferences in resource-constrained surveys. Since we were not aware of any prior work that empirically explored a way to determine the voice credit budget of QV, we assessed three different budgets in our QV survey experiment. Given that Likert scale surveys are the most widely adopted to date \cite{moors2016two}, we compared a QV survey to a Likert scale survey \cite{likert1932technique} in a resource-constrained setting. We identified two distinct types of survey questions in this setting: (1) choosing among $K$ independent options of one topic, and (2) choosing among $K$ dependent options that jointly contribute to the same topic. While the former is typical in public policy surveys, the latter is common to interface design surveys. Therefore, we studied the two types in two separate research questions:

% To offer an analogy, scenario one relates to choosing among ice cream, cakes, and puddings for dessert, while scenario two assesses what an individual cares more about when choosing ice cream -- the flavor, texture, or ingredients. Therefore, we analyzed them in two separate research questions:


\begin{itemize}
    \item[\textbf{RQ 1a}] How well do QV responses and Likert-scale responses align with the respondent's true preferences\footnote{That is, the phrase ``align with true preferences'' is equivalent to determining if the survey instrument is incentive-compatible.} in a survey where a survey respondent chooses among $K$ independent options of one topic? 
    
    \item[\textbf{RQ 1b}] How do variations in the number of voice credits available to QV survey respondents -- a small ($O(K)$), medium ($O(K^{1.5})$), and large ($O(K^2)$) budget -- impact this outcome?

    \item[\textbf{RQ 2}] How well do QV responses and Likert-scale responses align with the respondent's true preferences in a survey where the survey respondent chooses among $K$ dependent options that jointly contribute to the same topic? 
\end{itemize}

To answer RQ1, we designed a public polling survey, similar to those administered by Pew Research Center annually, to assess which social causes need more support, a typical scenario for choosing among $K$ independent options. To measure participants' true preferences on the same topic, we created an incentive-compatible donation task, in which participants should only donate more for cause A than cause B if they truly care more about cause A \cite{champ1997using}. In a randomized controlled experiment, participants completed either a 5-point Likert scale version or a QV version of the survey, and the donation task on the Amazon Mechanical Turk (MTurk) platform. We measured the similarity between each individual's survey result and their true preferences as reflected in the donation task and applied Bayesian analysis to compare the degree of similarity to true preferences in QV and Likert scale surveys.

In the case of choosing among $K$ dependent options that jointly contributed to the same topic in RQ2, we used an HCI user study scenario to understand how users made trade-offs across visual and audio elements in a video streaming experience given limited internet bandwidth. We used QV and 5-point Likert scale surveys to obtain participants' self-reported responses and created \tc{an} incentive-compatible product design and pricing task to elicit their willingness-to-pay for each video element \cite{roth1982incentive}. Similar to the first experiment, we conducted the randomized controlled experiment on MTurk and analyzed the results using Bayesian analysis.

\textbf{Contributions} This work contributes to the extensive body of work on survey techniques that elicit self-reported preferences in resource-constrained decision-making in two ways: we find an improved accuracy of responses via QV compared to the Likert scale norm, and we extend the use of QV to the HCI user study domain.

\begin{description}
\item[QV more accurately elicited true preferences:] We show empirically that QV with a medium (i.e., $O(K^{1.5})$) to large (i.e., $O(K^{2})$) voice credit budget elicited true preferences more accurately than Likert scale responses when the survey respondents were asked to either (1) choose among $K$ independent options of the same topic, or (2) choose among $K$ dependent options jointly contributing to the same topic, under resource constraints. \tc{Unlike} prior empirical work that compared the characteristics of QV and Likert scale responses \cite{quarfoot2017quadratic, naylor2017first}, we focused on accurate preference elicitation. Based on two carefully-designed randomized controlled experiments, our Bayesian analysis showed that QV responses aligned significantly closer to participants' responses in an incentive-compatible task \tc{than the} 5-point Likert scale responses with a medium to high effect size (0.56 for RQ1 and 0.51 for RQ2). This finding is important because it shows that QV, a computational-powered alternative survey approach that combines ratings and rankings, \tc{can} assist resource-constrained decision-making with more accurate self-reported responses.
\item[Application of QV to HCI:] We extend QV's application scenario from typical public policy and education research to a problem setting familiar to the CSCW community: a prototypical HCI user study. The limited prior empirical studies and applications of QV focused mostly on public policy \cite{quarfoot2017quadratic, colorado_qv} and education research \cite{naylor2017first}. To the best of our knowledge, no prior HCI study has applied QV in an online survey. We designed an HCI user study with a research question that involved resource-constrained decisions and showed that a QV survey elicited users' preferences on an HCI-related research question more accurately than a Likert scale survey, a norm in the HCI community \cite{ledo2018evaluation}. Our experiment results, QV survey design, and QV interface serve as a stepping stone for HCI researchers to further explore the use of this surveying method in their studies and encourage decision-makers from other fields to consider QV as a promising alternative.
\end{description}

While our study demonstrates the potential of QV as a computational tool to facilitate truthful preference elicitation, the goal of this research is \textit{not} to convince decision-makers to replace their Likert scale surveys with QV surveys entirely. Indeed, in cases requiring the use of \tc{paper-based} responses (e.g., when technological aids are unavailable), when the survey involves a list of unrelated options, or when the survey outcome is not resource-constrained,  QV may not be an appropriate option. Instead, our goal with this paper is to show that in resource-constrained scenarios of our interest in this paper that need both ratings and rankings, QV may prove to be a compelling alternative to Likert scale when conducting online surveys.  









% we pose many open questions to prompt further conversations in the community, and encourage decision-makers to carefully evaluate QV's strengths and weaknesses (e.g., higher cognitive cost, less known to participants) prior to adoption.
% accuracy
% wider application domains
% ask participants to express their attitudes using a scale for a given topic or question on a scale of 1 to 5. Researchers would use collected responses to make their decision. This paper focuses on collective decision-making tasks aimed at eliciting group preferences among a set of choices among $K$ options. Questions such as: ``Which interface do users like most?'', ``What are some products that customers are satisfied with?'', and  ``What public policies do constituents feel more important?'' are examples of eliciting group preferences among $K$ options.


% \textit{RQ1.} How do results from QV, Likert survey align with people's incentive-compatible behavior when surveying societal issues? 

% We designed an experiment to compare how participants donate with QV and Likert survey results. We analyzed data using Bayesian analysis and qualitative analysis. We hypothesized that QV results better portray individuals' true beliefs as reflected by their monetary donations.

% \textit{RQ2.} How do results from QV and Likert-scale align with people's incentive-compatible behavior with a choice of HCI research questions? 
% To the best of our knowledge, no HCI studies used QV in their studies. Similar to the first experiment, we compare results from QV and Likert surveys with participant's behaviors in a video-element HCI experiment. We hypothesize that QV more accurately reflects the intensity of individuals' true beliefs. 

% \textit{RQ3.} How do different quantities of voice credits impact the results of QV empirically?
% We did not find studies on how many voice credits a participant should receive for QV in related literature. In conjunction with RQ1, we provide participants with a different number of voice credits. We suspect different outcomes from QV when participants have a different number of voice credits.


% is prone to reduced data quality. 
% Since each statement of Likert survey is independent when answering, \textcite{alwin1985measurement} found participants respond with the first acceptable answer instead of compiling an optimal solution. Another fallacy follows similar logic where participants would express opinions to the extreme, exaggerating one's actual belief \cite{araujo2017much, vavreck2007exaggerated}. This is due to Likert survey's nature of not incorporating the notion of resource constraints in the real word where it actually exists, for example, in the case of allocating budget for government spending. Participants in the example could express having large budgets for all causes, which in reality, in not possible because the budget is limited. Some researchers tried to introduce constraints to rating based surveys, for instance knapsack voting, developed by \textcite{goel2015knapsack}, forces participants to elect agreement only if the sum of the cost for the selected items is less than a given budget.

% For example, ranking scales is a ranking-based technique created by \textcite{starch1923methods} in 1923. Participants ranks a list of options in the order of their preferences in a ranking scale survey. Though researchers believe ranking demonstrated better reflects individuals' accordance, they are harder to statistically analysis and more cognitive demanding to participants \cite{moors2016two}. In other words, people are forced to make distinctions between options for rank-based surveys and applying constraints, i.e., there can only be one item ranked first. When taking budget into considerations, researchers developed value-for-money-ranking which asks participants to rank according to not just how they value the options, but the value-cost ration of the options. The biggest drawback of rank-based methods is giving up the information of \textit{strength} among their standings, i.e., how far apart is the first preference compared with the second? 


% Since early 1980s when investigators start to distribute surveys using internet-connected devices, electronic-mediated surveys merely translate paper-based surveys onto a webpage. Researchers did not make noticeable changes on the \textit{survey mechanisms} that unleashes the power of computation. Given the computing power of computers today and the omnipresence of computers at any individual's hands, can a better surveying mechanism harness computational advantages to obtain more accurate survey results?

% In this paper, we examine an alternative, Quadratic Voting (QV), a relatively new voting mechanism proposed by \textcite{posner2018radical}, to elicit more truthful self-reported responses in resource-constrained collective decision making processes. 

% Distribution of surveys for resource-constrained decision making is pervasive in everyday life and these surveys have been effective at aggregating people's attitudes to form consensus. Think tanks design and deploy large-scale ordinal scale polls \cite{pew} to understand public opinions on government policies given limited government resources. Supermarkets collect individuals' shopping experiences across products because there are limited shelves \cite{nielsen}. 

%% electronic decision making surveys merely transform from paper, but hasn't done much extra with the computational power; 
% Before the 1980s, research surveys required large-scale in-person participation. Slowly, surveys evolved into mail-in, telephone, and, eventually, online formats. In the early 1980s, researchers introduced electronic surveys to individuals as internet-connected devices became accessible. \kk{How did people take these? Not too many people had ISP service in the 80s. Which companies administered them?} They are limited to international companies and fortune 500 in 1985. Electronic surveys aimed to reduce research costs, reduce human labor required to conducted surveys, and add additional flexibility in survey design\cite{kiesler1986response}. While the \textit{medium} of how researchers conducted the surveys moved from offline to online, researchers did not make noticeable changes on the \textit{survey mechanisms} that unleashes the power of computation. We argue that electronic-mediated surveys merely translate paper-based surveys onto a webpage. Given the computing power of computers today and the omnipresence of computers at any individual's hands, can a better surveying mechanism harness computational advantages to obtain more accurate survey results?

% QV is an example of utilizing the computational power, and have xxx, xxx, xxx proven benefits in previous work; 

% In this study, , given that quadratic computations are difficult to conduct on paper or verbally due to the required cognitive load.

% we also want to confirm if QV is "accurate" enough in soliciting people's opinions
% Since Likert is the most commonly used form of a survey; we compare QV's accuracy with Likert's; 
% Since surveys are deployed to accurately understand people's attitudes, this paper aims to understand how how well QV survey results align with real world behaviors and beliefs?. To the best of our knowledge, there have not been any empirical studies that investigated to what degree does QV results align with participants' underlying true preferences. 


%Self reported data, that is data supplied by the survey participant is at the core of such surveys across a wide range of disciplines, from medicine to social services \cite{stone1999science}. To address this, researchers, have designed different types of survey mechanisms throughout the past century. In 1923, 
%In another survey approach, pairwise preference, developed by \textcite{scheffe1952analysis} in 1952, participants compared two choices by stating their preferences in one of seven intensity levels. Researchers developed these methods to better elicit individual attitudes under different settings and for different goals. Many of the proposed methods, however, fail in capturing authentic signals due to self-reporting bias or <define it>.
% how to use a survey and when to use it is also important
%

% This paper offers three major contributions.
% First, we designed two empirical studies and found that QV aligns better with the participant's underlying preferences when surveying people's preferences among $K$ options. The first experiment looked at eliciting societal causes while the other experiment pertains to HCI. Both of these experiments used participant behaviors as a baseline to measure these differences, which we argue is much stronger than the direct comparison to Likert that prior studies lean on. The results allowed us to argue that researchers should harness computers' power to utilize and benefit from deploying QV in studies if the study aims to understand people's preferences among $K$ options.

% Second, we demonstrate that QV results depend on the voice credit budget. To the best of our knowledge, little did researchers discussed the effect of the number of voice credits in prior works. In this study, we discovered that participants need a large enough voice credit to obtain an accurate QV result.

% The third contribution demonstrates the application of QV in the HCI domain. We are not aware of prior studies in HCI that utilize QV, and we believe that our experiment and software deliverable provides a template to use QV in later research.

%%----------------------------%%




% understanding how to apply QV in real-world scenarios, what challenges QV might face, and design decisions that can influence QV participants.

% \textbf{Design Implication}
% TODO. Talk about interface, future work and insights.


% Our first question focused on whether QV consistently produces a different result from the Likert survey results in an HCI study. Although usually applied to surveys in the social sciences, we hypothesize that QV could produce a cleaner and more accurate result than a Likert survey in an alternative setting such as Human-Computer Interaction (HCI). We designed one pretest experiment and one follow-up experiment to verify our hypothesis. In the pretest, we replicated the study developed by Quarfoot et al. \cite{quarfoot2017quadratic} and swapped the content of the survey with an HCI security study by Leon et al. \cite{leon2013matters} to show that QV extracts more fine-grain information compared to Likert. Results from the pretest aligned with the results by Quarfoot et al., suggesting that QV works under scenarios outside of societal-focused settings. 

%%%%%%%% old text from thesis
% Commercial companies deploy online surveys to understand which improvements users would like to see prioritized for a similar constraint. 
% Other surveys can be found in shopping centers to . 
% Data scientists and product teams also use surveys for them to prioritize mission-critical issues that customers face when deploying a product upgrade. 
% All these examples demonstrate how prevalent groups and institutions used surveys to make decisions by gathering consensus from surveying individual's attitudes.
% Our second question regards how QV and Likert-scale align with an individual's preferences expressed via their behavior across societal issues. We carefully redesigned a new experiment involving a larger group of participants to compare the alignment of QV and Likert surveys with a person's actual behavior. In this experiment, participants expressed their attitudes across societal causes and were given the option to donate to charitable organizations associated with each cause. We took a Bayesian approach to analyze the statistical power and effect size of how closely the Likert Survey and QV align with one's donation distribution. The results found statistical evidence that QV outperforms Likert if there are enough voice credits. Our third question looked at whether a different number of voice credit budget impacts people's voting behavior in QV. We used both experiments, the pretest experiment, and the formal experiment to test this idea. We found through qualitative and quantitative analysis that QV surveys require a significant amount of voice credits. We describe these research question more thoroughly in Chapter \ref{RQ}.

% Since then, this surveying method has been fully adopted by researchers and marketers alike. 
% Some researchers attributed this to the survey's ease of use. However, this ease of use does not guarantee the ease of analysis. Researchers today easily misuse Likert surveys by applying the incorrect analysis methods \cite{bishop2015use} or misinterpreting the analysis results \cite{jamieson2004likert, pell2005use}, leading to doubtful findings. Examples of such includes calculating the mean and standard deviations to interpret the data collected through a Likert survey. Even when applied correctly, there is usually little justification for using Likert surveys over other research tools in their research. 

% and how well it aligns with other surveying methods such as Likert survey. Further, we did not find any empirical study examining whether a different amount of voice credits impacts QV's result.
% Researchers have compared the Likert survey with QV from empirical and theoretical perspectives \cite{quarfoot2017quadratic, naylor2017first}. Cavaille et al. argued that QV outperforms Likert surveys among a set of political and economic issues \cite{cavaille2018towards}. 

% Surveys have been a useful tool at aggregating 
% Research agencies, industry labs, or independent researchers often hope to understand how to allocate resources better or to address people's preferences more accurately. 
% Agencies collect these surveys of huge crowds as a form of collective decision making. 


%%%%%%% old text from QV draft
% Likert survey is one of the most widely used methods to obtain the participant's opinion in the realm of human-computer interaction. Survey participants would express a rating across a series of measurements --- \textit{Very agree to very disagree} or \textit{On a scale of 1 to 5} --- for a listed statement. Very often, these opinions help researchers or decision-makers uncover consensus  across a group of people. However, there had been findings of how researchers can easily misuse Likert surveys either applying incorrect analysis methods  \cite{bishop2015use} or misinterpreting the analysis results \cite{jamieson2004likert, pell2005use} leading to questionable findings.In addition,  many research papers do not explain the rational behind the use of  Likert surveys. In a community that adopted Likert surveys almost as the defacto standard, we ask a fundamental question: ``Is Likert-scale survey the ideal method to measure collective attitudes for decision making?''
% Research agencies, industry labs or independent researchers often want to understand how to better allocate resources or what to improve upon. This is an example of eliciting user preferences  among $K$ options. These opinions are collected among huge crowds, as a form of collective decision making. For example,  ordinal scale polls were designed to understand public opinions on government policy \cite{pew} because there is limited funding. Companies deploy online surveys  to understand how product users  feel about the features and services that needs further improvements because companies have limited time  to develop the next release. Physical surveys can be found  in shopping centers to collect an individual's experiences for products on the shelf because there are limited shelves. All these examples demonstrated how surveys are often tied to  making decisions  by gathering consensus from surveying individual's attitudes. 
% In this study, we look at an alternative method called Quadratic Voting (QV). Published in 2015, \textcite{posner2018radical} proposed Quadratic voting as a voting mechanism with approximate Pareto efficiency.Under this voting mechanism,voters were initially given a fixed amount of voice credits (VC).With the credits, individuals can purchase any number of votes to support any of the statementslisted on the ballot. However, the cost of each vote increases quadratically when voted toward the same option.The authors proved that this mechanism is more efficient at making a collective decision because it minimizes welfare loss.Since 2015, a few studiescompared Likert surveys with QV empirically and theoretically\cite{quarfoot2017quadratic, naylor2017first}.\textcite{cavaille2018towards} argues that QV reflects one's opinion better compared to Likert-scale surveys when surveying one's opinionamong a set of political and economic issues.Despite these findings,we are not aware of related works thatcompare Likert surveys and QVwith participants' underlying true preferences.Therefore, it is unclear whether or notand in what degreedoes QV results align with participants' behaviors.In addition, no current work, to the best of our knowledge,deployed QV in the area of HCI.
%\end{enumerate}
% To answer these research questions,
% we designed two experiments.
% The first experiment,
% designed to answer RQ1 and RQ3,
% is a between-subject study
% where participants express their attitudes
% among a set of societal causes using 
% QV and Likert surveys
% and then donate
% to organizations relevant to these organizations.
% The second experiment 
% created an HCI study environment,
% aimed to answer RQ2,
% where participants were asked about 
% opinions among different video elements
% and their opinions using QV and Likert surveys.
% Our results showed that both experiments support
% QV in providing a clean and efficient way
% compared to Likert surveys
% at eliciting participant's true preferences.
