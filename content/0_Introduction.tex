\section{Introduction}

{\change{Surveys are a common instrument to gauge opinion for scholars in the CSCW community, the social sciences, and many other research areas. The Likert ordinal scale~\cite{likert1932technique} is a familiar instrument in surveys that elicit ratings, where survey takers indicate their level of agreement, satisfaction, or perceived importance along an intensity scale~\cite{moors2016two}. Nearly a hundred years after~\textcite{likert1932technique}, we find the ordinal scale in widespread use in online surveys, including those on Qualtrics, SurveyMonkey, and Google Forms platforms. Prior work indicates that self-reports by survey-takers, including responses recorded via an ordinal scale, can be inaccurate~\cite{araujo2017much, vavreck2007exaggerated}. Today, scholars utilize online surveys to reach a larger audience than paper-based surveys. However, the issue of inaccurate self-reports persists since many online incarnations of these ordinal scales are virtually indistinguishable from Likert scale surveys administered on paper-based forms nearly a century ago. Therefore, in this paper, we ask: \textit{Do the affordances of modern computer interfaces, coupled with a modern computer's ability to make real-time calculations, allow us to develop novel, screen-based techniques to elicit more truthful responses from survey-takers?} By ``truthful-responses,'' we mean that the survey instrument is incentive-compatible---truth-telling is a dominant strategy for the respondent. Throughout this paper, we shall use truthful-response, an informal phrase , instead of the more formal ``incentive-compatible,'' in use in Game theory, to intuitively capture our intent.

We examine our motivating question within the context of surveys where the survey creator made decisions by aggregating respondents' elicited opinions. At the same time, these decisions could potentially impact these survey respondents. Examples of such surveys include those that ask respondents about which policies the government ought to pursue~\cite{pew_spending}, ask users for their opinions on different interface designs~\cite{ledo2018evaluation}, and ask customers what products to stock at grocery stores~\cite{nielsen}. The decision-maker (i.e., the group or the organization that uses the survey results) then uses the survey aggregated responses to implement one of the several options presented to the survey-takers. These decision-makers often cannot implement all the choices they gave to the survey-takers due to limited resources such as money, time, space, and human resources. In this paper, we term such surveys, where respondents help the decision-maker choose among a set of options under limited resources, \textit{resource-constrained surveys}. More specifically, we focus on resource-constrained surveys where respondents have some utility in the survey outcome (e.g., the specific policy that the government adopts), even though the exact value of the utility or the time to receive that utility may remain uncertain. In contrast, surveys that, for example, aim to develop rankings by asking respondents to identify their favorite hobby, place to visit, or their favorite celebrity, are not survey types that we consider in this paper. 

In resource-constrained surveys, decision-makers ask the survey respondents two questions: \textit{how much} do you prefer each option; how would you \textit{prioritize} (i.e., rank) the options? While in the past century, decision-makers have used ratings and rank-based survey instruments to answer such questions, the instruments answered \emph{one} of the two questions, but not both~\cite{moors2016two}.  Next, we examine two common survey techniques: one based on ratings and the other based on rankings.

In a rating-based survey, survey takers indicate their level of agreement, satisfaction, or perceived importance along an intensity scale~\cite{moors2016two}. The Likert scale is the most commonly used rating-based technique that elicits the \textit{strength} of a survey taker's preferences~\cite{likert1932technique}. Rating-based surveys are easy to understand and administer; this may explain why the Likert scale is the current norm in many survey settings. The Likert ordinal scale is problematic in the resource-constrained survey types that are of interest in this paper. Consider, for example, the Pew Research Center annual surveys with a 3-point Likert-like scale that ask how participants would budget federal government resources~\cite{pew_spending}. Respondents in such a survey can freely choose  ``increase spending'' on the scale for \textit{all} government program areas. However, a government with limited resources cannot operationalize this option. This behavior is known as ``extreme responding'' bias~\cite{batchelor2016extreme, furnham1986response, meisenberg2008acquiescent}. A Likert scale survey does not safeguard against exaggerated or unrealistic preferences~\cite{araujo2017much, vavreck2007exaggerated}. Furthermore, it may inaccurately elicit how participants \textit{prioritize} the options since each option is independently assessed~\cite{alwin1985measurement}. 

Ranking-based surveys focus on understanding how participants \textit{prioritize} among the possible options~\cite{moors2016two}. For example, participatory budgeting~\cite{cabannes2004participatory,goel2015knapsack,Goel2016, Lee2014, benade2020preference} is a promising alternative for governments to ask citizens to prioritize resource allocation.  In participatory budgeting, voters are asked to identify a subset $I$ among projects $P$ on which they would like the government to allocate, and each project $i \in P$ has a fixed implementation cost $c_i$ known to the survey taker. The survey-taker is also given the total budget available $B$ to the decision-maker (e.g., local city council) and each respondent is asked to pick a subset $I \in P$ of projects such that the total costs $\sum_{i \in I} c_i \leqslant B$ stays within the budget. There are two main challenges with ranking-based surveys. First, ranking-based surveys cannot elicit the \textit{strength} of a participant's preferences, i.e., the degree to which participant prefers option A over B. Second, ranking surveys that use participatory budgeting may be less easy to adapt to CSCW or social science surveys, since the decision-maker, who creates the survey, has to assign implementation costs to each option and an overall budget. For example, in a survey about interface design, the decision-maker would need to assign costs to each interface element on which they are eliciting an opinion. Furthermore, unlike the typical participatory budgeting scenario, where project costs involve allocating \textit{the respondent's tax dollars,} in typical social science and HCI surveys, the implementation of an option involves no obvious cost to a participant. 

This paper examines Quadratic Voting (QV) as a possible preference elicitation mechanism in online surveys. \textcite{posner2018radical} recently advocated for the use of QV as an alternative for the traditional one-person-one-vote system. In the \textcite{posner2018radical} formulation, each person has access to a fixed number of voice credits $B$ (i.e., similar to the budget in participatory budgeting) to allocate across options. Then, each person can cast more than one vote (for, or against) for each option, with the proviso that the votes have a quadratic cost. Thus, casting $n_k$ votes on option $k$ would cost $c(n_k) \propto n_k^2$ in voice credits. Furthermore, the total cost in voice credits across all options cannot exceed the budget $B$. Therefore, a person casting positive or negative votes across $k$ options has to satisfy $\sum_k n_k^2 \leqslant B$, where $n_k$ is the number of votes cast on option $k$. 

What makes QV of interest to the CSCW community that uses online surveying tools is that it appears to straddle the two familiar elicitation mechanisms of rating and ranking. While respondents can cast any number of voice credits modulo budgeting constraints on an option and thus indicating the strength of a preference, unlike Likert, they \textit{cannot} allocate a high number of votes to \textit{all} options. Notice that the quadratic vote costs imply that every additional vote on an option, say a change from $n$ to $n+1$ votes, increases the marginal cost by $(n+1)^2-n^2=2n+1$ voice credits. Due to the quadratic budget constraint, QV gently nudges the respondents to prioritize among the options. Prior work further showed that aggregated QV results of an option in a sample approximate a normal distribution in contrast to  the ``extreme responding'' behavior found in Likert surveys~\cite{quarfoot2017quadratic}.~\textcite{Lalley2018} also proved \textit{robust optimality} properties of QV, suggesting that QV may be incentive-compatible (i.e., truth-telling is the dominant strategy for the respondent). Yet, to our best knowledge, this optimality assertion lacks empirical validation in the survey contexts of interest to this paper. Since QV requires real-time calculations to let the respondent know if they are allocating voice credits across options within the overall budget, QV requires a computational engine. The ubiquity of smartphones and tablets, the desire for the CSCW community to reach a wide audience through online surveys, and QV's robust optimality all suggest that QV is worth investigating as an alternative online survey instrument. Though there were many unanswered questions---from designing QV interfaces to communicating what QV is to users---we believe examining whether QV elicits more accurate responses from participants than Likert scale surveys is an important first step.}}


In our study, we examined QV's ability to elicit true preferences in resource-constrained {\change{surveys. Here we define a participant's \textit{true preference} as their incentive-compatible behaviors. Since we were not aware of any prior work that empirically explored a way to determine the voice credit budget of QV, we assessed three different budgets in our QV survey experiment.}} Given that Likert scale surveys are the most widely adopted to date~\cite{moors2016two}, we compared a QV survey to a Likert scale survey~\cite{likert1932technique} in a resource-constrained setting. We identified two distinct types of survey questions in this setting: (1) choosing among $K$ independent options of one topic, and (2) choosing among $K$ dependent options that jointly contribute to the same topic. {\change{While the former is typical in public policy surveys, the latter is common to interface design surveys. Therefore, we studied the two types}} in two separate research questions:

\begin{itemize}
    {\change{\item[\textbf{RQ 1a}] How well do QV responses and Likert-scale responses align with the respondent's true preferences\footnote{That is, the phrase ``align with true preferences'' is equivalent to determining if the survey instrument is incentive-compatible.} in}} a survey where a survey respondent chooses among $K$ independent options of one topic? 
    
    {\change{\item[\textbf{RQ 1b}] How do variations in the number of voice credits available to QV survey respondents -- a small ($O(K)$), medium ($O(K^{1.5})$), and large ($O(K^2)$) budget -- impact this outcome?}}

    {\change{\item[\textbf{RQ 2}] How well do QV responses and Likert-scale responses align with the respondent's true preferences}} in a survey where the survey respondent chooses among $K$ dependent options that jointly contribute to the same topic? 
\end{itemize}

To answer RQ1, we designed a public polling survey{\change{, similar to those administered by Pew Research Center annually, to assess}} which social causes need more support, a typical scenario for choosing among $K$ independent options. To measure participants' true preferences on the same topic, we created an incentive-compatible donation task, in which participants should only donate more for cause A than cause B if they truly care more about cause A~\cite{champ1997using}. {\change{In a randomized controlled experiment, participants}} completed either a 5-point Likert scale version or a QV version of the survey and the donation task {\change{on the Amazon Mechanical Turk (MTurk) platform.}} We measured the similarity between each individual's survey result and their true preferences as reflected in the donation task and applied Bayesian analysis to compare the degree of similarity to true preferences in QV and Likert scale surveys.

In the case of choosing among $K$ dependent options that jointly contributed to the same topic in RQ2, we used an HCI user study scenario to understand how users made trade-offs across visual and audio elements in a video streaming experience given limited internet bandwidth. We used QV and 5-point Likert scale surveys to obtain participants' self-reported responses and {\change{created an }}incentive-compatible product design and pricing task to elicit their willingness-to-pay for each video element~\cite{roth1982incentive}. Similar to the first experiment, we conducted the randomized controlled experiment on MTurk and analyzed the results {\change{using}} Bayesian analysis.

\textbf{Contributions} This work contributes to the extensive body of work on survey techniques that elicit self-reported preferences in resource-constrained decision-making in two ways: we find an improved accuracy of responses via QV compared to the Likert scale norm, and we extend the use of QV to the HCI user study domain.

\begin{description}
\item[QV {\change{more accurately elicited}} true preferences:] We empirically {\change{showed}} that QV with a medium (i.e., $O(K^{1.5})$) to large (i.e., $O(K^{2})$) voice credit budget elicited true preferences more accurately than Likert scale responses when the survey respondents were asked to either (1) choose among $K$ independent options of the same topic, or (2) choose among $K$ dependent options jointly contributing to the same topic, under resource constraints. Unlike prior empirical work that compared the characteristics of QV and Likert scale responses~\cite{quarfoot2017quadratic, naylor2017first}, we focused on accurate preference elicitation. Based on two carefully-designed randomized controlled experiments, our Bayesian analysis showed that QV responses aligned significantly closer to participants' responses in an incentive-compatible task {\change{than the}} 5-point Likert scale responses with a medium to high effect size {\change{(0.56 for RQ1 and 0.51 for RQ2)}}. This finding is important because it shows that QV, a computational-powered alternative survey approach that combines ratings and rankings, {\change{can}} assist resource-constrained decision-making with more accurate self-reported responses.

\item[Application of QV to HCI:] We extend QV's application scenario from typical public policy and education research to a problem setting familiar to the CSCW community: a prototypical HCI user study. The limited {\change{prior}} empirical studies and applications of QV focused mostly on public policy~\cite{quarfoot2017quadratic, colorado_qv} and education research~\cite{naylor2017first}. To the best of our knowledge, no {\change{prior}} HCI study has applied QV {\change{in an online}} survey. We designed an HCI user study with a research question that involved resource-constrained decisions. {\change{Compared to a Likert scale survey, a norm in the HCI community~\cite{ledo2018evaluation}, we show}} that a QV survey {\change{more accurately elicited}} users' preferences on an HCI-related research question. Our experiment results, QV survey design, and QV interface serve as a stepping stone for HCI researchers to further explore the use of this surveying method in their studies and encourage decision-makers from other fields to consider QV as a promising alternative.
\end{description}

While our study demonstrates the potential of QV as a computational tool to facilitate truthful preference elicitation, the goal of this research is \textit{not} to convince decision-makers to replace their Likert scale surveys with QV surveys entirely. Indeed, in cases when the survey requires paper-based responses (e.g., when technological aids are unavailable), when the survey involves a list of unrelated options, {\change{or when the survey outcome is not resource-constrained,}} QV may not be an appropriate option. Instead, our goal with this paper is to show that QV may be a compelling alternative to the Likert scale when conducting online surveys {\change{under resource-constrained scenarios where it's beneficial to elicit both the respondents' ratings and rankings preferences.}}  
