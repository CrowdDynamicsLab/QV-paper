\section{Introduction}

%% Survey, decision making, and everyone
Surveys are useful tools that aggregate individuals' attitudes among a group of participants to form consensus when there are constraints in resource distribution. 
Large-scale ordinal scale polls designed by think tanks aimed to understand public opinions on government policies ~\cite{pew} because there are limited government resources. 
Supermarkets collect individuals' shopping experiences across products because there are limited shelves.
Distribution of these surveys for resource-constraint decision making is pervasive in everyday life.

%% electronic decision making surveys merely transform from paper, but hasn't done much extra with the computational power; 
In the past, research surveys use to require large-scale in-person participation. 
Slowly, surveys evolved into mail-in, telephone, and, eventually, online formats.
In the early 1980s, researchers introduced electronic surveys to individuals as they began having access to internet-connected computers. 
Electronic surveys aimed to reduce research costs, reduce the amount of human labor required to conducted surveys, and add additional flexibility in survey design. 
While the \textit{medium} of how researchers conducted the surveys moved offline to online, researchers did not make noticeable changes on the \textit{survey mechanisms} that unleashes the power of computation. 
We argue that electronic-meditated surveys merely translate a paper-based survey onto a webpage.
Given the computing power of computers today and the omnipresence of computers at any individual's hands, can a better surveying mechanism harness computational advantages project more accurate survey results?

% QV is an example of utilizing the computational power, and have xxx, xxx, xxx proven benefits in previous work; 
Quadratic voting (QV) is an alternative collaborative decision-making surveying method developed by Weyl at al. \cite{posner2018radical} in 2015. 
This voting mechanism works as follows: Each QV participant receives an equal budget of voice credits. 
There are multiple options on the ballot. Participants can vote any number of votes in favor of or against these options.
However, a quadratic cost is associated with the number of votes the participant cast toward the given option. For example, participants will spend one voice credit for one vote and requiring four voice credits for two votes. The more votes a participant cast for an option, the more costly the votes were. Weyl at al. argued that this mechanism is more efficient than a 1-person-1-vote system to make a collective decision because it minimizes welfare loss. Previous work demonstrated that QV provides normal-distributed outcomes compared to Likert scale survey results regarding polarized topics \cite{quarfoot2017quadratic}. 
We argue that researchers should harness computers' power to utilize and benefit from deploying QV in studies, given that quadratic computations are difficult to conduct on paper or verbally due to the required cognitive load.

% we also want to confirm if QV is "accurate" enough in soliciting people's opinions
% Since Likert is the most commonly used form of a survey; we compare QV's accuracy with Likert's; 
Since the goal of conducting surveys was to understand people's attitudes accurately, this paper aims to understand how QV aligns with what participants genuinely believe and how they act. To the best of our knowledge, there have not been any empirical studies that investigated to what degree does QV results align with participants' underlying true preferences. In this paper, we focus our comparison of QV on Likert surveys. Likert surveys, one of the most widely used surveying technique invented in 1932, ask participants to express their attitudes using a scale, for example, on a scale of 1 to 5, for a given topic or option. Researchers would use collected responses to make their decision. Scope wise, this paper focuses on collective decision-making tasks aimed at eliciting group preferences among a set of choices among $K$ options. Questions such as: ``Which interface do users like most?'', ``What are some products that customers are satisfied with?'', and  ``What public policies do constituents feel more important?'' are examples of eliciting group preferences among $K$ options.

In this paper, we ask three research questions: 

\textit{RQ1.} How does results from QV, Likert survey align with people's behavior when surveying societal issues? 

Unlike prior works, we designed an experiment that collects the participant's donation behaviors, comparing it with survey results of QV and Likert Surveys. We applied bayesian analysis and qualitative analysis to the results. We hypothesized that QV better reflects individuals' true beliefs. 

\textit{RQ2.} How do results from QV, Likert-scale align with people's behavior with a choice of HCI research questions? 
To the best of our knowledge, no  HCI studies used QV in their studies. Similar to the first experiment, we compare results from QV and Likert surveys with participant's behaviors for a video-element HCI experiment. We hypothesis that QV more accurately reflects the intensity of individuals' true beliefs. 

\textit{RQ3.} How do different amounts of voice credits impact the results of QV empirically?
In related literature, we did not find studies on how many voice credits a participant should receive for QV. In conjunction with RQ1, we provide participants with a different number of voice credits. We suspect different outcomes from QV when participants have a different number of voice credits.


\textbf{Contributions}
This paper contains three major contributions.
First, we demonstrated empirical studies that QV aligns better with the participant's underlying preferences when surveying people's preferences among $K$ options. We conducted two experiments using different experiment designs and applied to a different study domain only to show similar results. Both of these experiments used participant behaviors as a baseline to measure these differences, which we argue is much stronger than direct comparison to Likert that prior studies lean on. The results mean that researchers should consider using QV over the Likert survey to understand people's preferences among $K$ options. 
The second contribution is demonstrating that having a different voice credit budget size could impact the QV results. To the best of our knowledge, little did researchers discussed the effect of the number of voice credits in prior works. In this study, we discovered that participants need a large enough voice credit to obtain an accurate QV result.
The third contribution is that we demonstrated the application of QV in the HCI domain. We are not aware of prior studies in HCI that utilize QV, and we believe that our experiment and software deliverable provides a template to use QV in later research.

These findings are critical to QV applications in the wild. It is important to stress that the goal of this research is \textit{not} to convince a replacement of Likert Surveys. Instead, we believe QV as an additional tool inside the researcher's toolkit to facilitate better decision making. We also think this study opens the door to many future directions, such as investigating interface designs for QV.

% understanding how to apply QV in real-world scenarios, what challenges QV might face, and design decisions that can influence QV participants.

% \textbf{Design Implication}
% TODO. Talk about interface, future work and insights.


% Our first question focused on whether QV consistently produces a different result from the Likert survey results in an HCI study. Although usually applied to surveys in the social sciences, we hypothesize that QV could produce a cleaner and more accurate result than a Likert survey in an alternative setting such as Human-Computer Interaction (HCI). We designed one pretest experiment and one follow-up experiment to verify our hypothesis. In the pretest, we replicated the study developed by Quarfoot et al. \cite{quarfoot2017quadratic} and swapped the content of the survey with an HCI security study by Leon et al. \cite{leon2013matters} to show that QV extracts more fine-grain information compared to Likert. Results from the pretest aligned with the results by Quarfoot et al., suggesting that QV works under scenarios outside of societal-focused settings. 

% 



%%%%%%%% old text from thesis
% Commercial companies deploy online surveys to understand which improvements users would like to see prioritized for a similar constraint. 
% Other surveys can be found in shopping centers to . 
% Data scientists and product teams also use surveys for them to prioritize mission-critical issues that customers face when deploying a product upgrade. 
% All these examples demonstrate how prevalent groups and institutions used surveys to make decisions by gathering consensus from surveying individual's attitudes.
% Our second question regards how QV and Likert-scale align with an individual's preferences expressed via their behavior across societal issues. We carefully redesigned a new experiment involving a larger group of participants to compare the alignment of QV and Likert surveys with a person's actual behavior. In this experiment, participants expressed their attitudes across societal causes and were given the option to donate to charitable organizations associated with each cause. We took a Bayesian approach to analyze the statistical power and effect size of how closely the Likert Survey and QV align with one's donation distribution. The results found statistical evidence that QV outperforms Likert if there are enough voice credits. Our third question looked at whether a different number of voice credit budget impacts people's voting behavior in QV. We used both experiments, the pretest experiment, and the formal experiment to test this idea. We found through qualitative and quantitative analysis that QV surveys require a significant amount of voice credits. We describe these research question more thoroughly in Chapter \ref{RQ}.

% Since then, this surveying method has been fully adopted by researchers and marketers alike. 
% Some researchers attributed this to the survey's ease of use. However, this ease of use does not guarantee the ease of analysis. Researchers today easily misuse Likert surveys by applying the incorrect analysis methods \cite{bishop2015use} or misinterpreting the analysis results \cite{jamieson2004likert, pell2005use}, leading to doubtful findings. Examples of such includes calculating the mean and standard deviations to interpret the data collected through a Likert survey. Even when applied correctly, there is usually little justification for using Likert surveys over other research tools in their research. 

% and how well it aligns with other surveying methods such as Likert survey. Further, we did not find any empirical study examining whether a different amount of voice credits impacts QV's result.
% Researchers have compared the Likert survey with QV from empirical and theoretical perspectives \cite{quarfoot2017quadratic, naylor2017first}. Cavaille et al. argued that QV outperforms Likert surveys among a set of political and economic issues \cite{cavaille2018towards}. 

% Surveys have been a useful tool at aggregating 
% Research agencies, industry labs, or independent researchers often hope to understand how to allocate resources better or to address people's preferences more accurately. 
% Agencies collect these surveys of huge crowds as a form of collective decision making. 


%%%%%%% old text from QV draft
% Likert survey is one of the most widely used methods to obtain the participant's opinion in the realm of human-computer interaction. Survey participants would express a rating across a series of measurements --- \textit{Very agree to very disagree} or \textit{On a scale of 1 to 5} --- for a listed statement. Very often, these opinions help researchers or decision-makers uncover consensus  across a group of people. However, there had been findings of how researchers can easily misuse Likert surveys either applying incorrect analysis methods  \cite{bishop2015use} or misinterpreting the analysis results \cite{jamieson2004likert, pell2005use} leading to questionable findings.In addition,  many research papers do not explain the rational behind the use of  Likert surveys. In a community that adopted Likert surveys almost as the defacto standard, we ask a fundamental question: ``Is Likert-scale survey the ideal method to measure collective attitudes for decision making?''
% Research agencies, industry labs or independent researchers often want to understand how to better allocate resources or what to improve upon. This is an example of eliciting user preferences  among $K$ options. These opinions are collected among huge crowds, as a form of collective decision making. For example,  ordinal scale polls were designed to understand public opinions on government policy \cite{pew} because there is limited funding. Companies deploy online surveys  to understand how product users  feel about the features and services that needs further improvements because companies have limited time  to develop the next release. Physical surveys can be found  in shopping centers to collect an individual's experiences for products on the shelf because there are limited shelves. All these examples demonstrated how surveys are often tied to  making decisions  by gathering consensus from surveying individual's attitudes. 
% In this study, we look at an alternative method called Quadratic Voting (QV). Published in 2015, \textcite{posner2018radical} proposed Quadratic voting as a voting mechanism with approximate Pareto efficiency.Under this voting mechanism,voters were initially given a fixed amount of voice credits (VC).With the credits, individuals can purchase any number of votes to support any of the statementslisted on the ballot. However, the cost of each vote increases quadratically when voted toward the same option.The authors proved that this mechanism is more efficient at making a collective decision because it minimizes welfare loss.Since 2015, a few studiescompared Likert surveys with QV empirically and theoretically\cite{quarfoot2017quadratic, naylor2017first}.\textcite{cavaille2018towards} argues that QV reflects one's opinion better compared to Likert-scale surveys when surveying one's opinionamong a set of political and economic issues.Despite these findings,we are not aware of related works thatcompare Likert surveys and QVwith participants' underlying true preferences.Therefore, it is unclear whether or notand in what degreedoes QV results align with participants' behaviors.In addition, no current work, to the best of our knowledge,deployed QV in the area of HCI.
%\end{enumerate}
% To answer these research questions,
% we designed two experiments.
% The first experiment,
% designed to answer RQ1 and RQ3,
% is a between-subject study
% where participants express their attitudes
% among a set of societal causes using 
% QV and Likert surveys
% and then donate
% to organizations relevant to these organizations.
% The second experiment 
% created an HCI study environment,
% aimed to answer RQ2,
% where participants were asked about 
% opinions among different video elements
% and their opinions using QV and Likert surveys.
% Our results showed that both experiments support
% QV in providing a clean and efficient way
% compared to Likert surveys
% at eliciting participant's true preferences.
