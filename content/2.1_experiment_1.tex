\section{Methods -- Experiment One: Choosing among Independent Options}~\label{method_exp1}

We designed a between-subjects randomized controlled experiment to answer our first research question: {\change{\begin{quote} RQ1a: How well do QV responses and Likert-scale responses align with the respondent's true preferences\footnote{That is, the phrase ``align with true preferences'' is equivalent to determining if the survey instrument is incentive-compatible.} in a survey where a survey respondent chooses among $K$ independent options of one topic? \end{quote} \begin{quote} RQ1b: How do variations in the number of voice credits available to QV survey respondents---a small ($O(K)$), medium ($O(K^{1.5})$), and large ($O(K^2)$) budget---impact this outcome? \end{quote} }}The study was in the context of a public opinion polling to understand participants' preferences towards various societal causes, such as the environment, education, veterans. We focused on the topic of societal causes because public goods and resource allocation across causes is a problem relevant to every citizen. Since resources are limited in public sectors, this problem is a typical example of choosing among $K$ independent options. Each participant completed one of the two kinds of surveys on the importance of the nine societal causes and a donation task. We detail the flow of our experiment in this section.


\subsection{Participants Recruitment}
We recruited participants located in the US from Amazon Mechanical Turk (MTurk) through the CloudResearch platform~\cite{litman2017turkprime} {\change{in the 1st quarter of 2020}}. {\change{The MTurk population skews towards the younger and higher educated population~\cite{berinsky2012evaluating}, but an ideal surveying tool should be inclusive and serve people of all ages and education levels. Therefore, we did our best to align the participants' age and education level distribution to the latest available United States census estimates in 2018~\cite{census2018}.}} We randomly assigned them into two groups: the Likert group and the QV group. Participants in the Likert group had a median completion time of 11.5 minutes and received \$0.75, and those in the QV group received \$2.5 due to a longer study length, with a median completion time of 20 minutes 56 seconds.

\subsection{Experimental Flow}
\begin{figure}[htpb]
    \centering
    \includegraphics[width=\textwidth, keepaspectratio=true]{content/image/exp1_flow.pdf}
    \caption{
        Experiment one was a between-subjects experiment. We randomly assigned participants into two groups. Participants that took the upper path were in the Likert Group, and they expressed their attitudes toward various social causes through a five-point Likert scale. QV group participants reported their attitudes through two of the three variations of QV survey, with $36$, $108$, and $324$ voice credits respectively.
    }
    \label{fig:exp1_image_flow}
\end{figure}

\Cref{fig:exp1_image_flow} summarizes the experimental procedure. The experiment consisted of four steps: 1) demographic survey, 2) Likert or QV survey, 3) {\change{a filler task}}, and 4) a donation task. We provide the complete experimental protocol in the supplementary materials.

\textbf{Step 1: Demographic Survey.} Participants joined the study under the impression that the goal of the study was to understand their opinions towards \textit{the importance of various societal causes}. After signing the consent form, participants completed a demographic survey that asked for their gender, ethnicity, age, household income level, education level, and current occupation.

\textbf{Step 2.1: Group 1 -- Likert Scale Survey.} The experiment randomly assigned some of the participants to the Likert group, shown in the upper path of \Cref{fig:exp1_image_flow}. In the prompt, we explicitly told the participants that there are limited resources in society, and that people have different preferences at allocating resources to various societal causes. The survey focused on nine societal issues, including 1) pets and animals, 2) arts, culture and humanities, 3) education, 4) environment, 5) health, 6) human services, 7) international causes, 8) faith and spiritual causes, and 9) veterans\footnote{For detailed definitions of each cause, please refer to~\Cref{cause_def}}. We derived the nine societal causes from the categorization of charity groups on Amazon Smile\footnote{https://smile.amazon.com/}, a popular donation website that has accumulated over 100 million dollars of donations.

We asked the participants to rate each of the nine societal issues on a 5-point Likert scale: ``For each of the issues listed below, how important do you think the issue is to you and that more effort and resources should be contributed towards the issue?''. The 5-point Likert options ranged from ``Very important'' to ``Very Unimportant.'' While there are a variety of Likert scales (3-point, 5-point, 7-point, and even 11-point), we used a 5-point Likert scale since it is one of the most commonly used scale~\cite{dawes2008data}.

\textbf{Step 2.2: Group 2 -- QV Survey} The QV group took the lower path in \Cref{fig:exp1_image_flow}. We first asked participants to watch a pre-recorded tutorial video that introduced how QV works and how to use our QV interface since we did not expect participants to know about QV before taking part in the study, as opposed to Likert scale. Participants had unlimited time to interact with a demo QV interface to familiarize themselves with QV. To ensure that the participants paid attention and understood QV, they needed to correctly answer at least three of the five multiple-choice quiz questions related to QV to continue with the study.

Once they passed the quiz, participants encountered two of the three versions of the QV surveys at random. The three versions of QV had 36, 108, and 324 voice credits, respectively. We showed them the same prompt as in the Likert group and instructed them to answer the same question for the nine identical causes, but with QV instead. Participants cast positive votes for causes they considered important and vice versa.

To our knowledge, no prior work discussed about how to decide on the voice credit budget in QV empirically. Therefore, we designed three versions of the QV survey to answer the second question in RQ1: how does the amount of voice credits in QV impact QV's ability to elicit true preferences? To examine how larger voice credit budgets impact people's choices, we set an exponential increase based on the number of options ($K$) on the survey ($O(K)$, $O(K^{1.5})$ to $O(K^2)$). We investigated three levels of voice credits: $K \times O$, $K^{1.5} \times O$, and $K^2 \times O$, where $K$ is the number of options in the survey and $O$ is the number of credits required to express an attitude in QV that is equivalent to the strongest attitude in a 5-point Likert scale survey, where ``Neutral" in Likert = 0 vote in QV and one level in Likert = one vote in QV. In this experiment, $K=9$ corresponded to the $9$ societal causes. We used a 5-point Likert scale survey with extreme levels at $\pm 2$; hence each participant needed four voice credits ($2^2=4$) to express the extreme Likert levels in QV, which translated to $O=4$. Thus, the three levels of voice credits in the experiment were {\change{$9\times4=36$ (QV36), $9^1.5\times 4=108$ (QV108), and $9^2\times 4=324$ (QV324)}}. In all three cases, participants could afford to express any results from Likert in the form of QV. 
{\change{
In addition, we asked participants to complete two of the three QV surveys to help understand how an increase or decrease in voice credits impacted their response behavior. To minimize the learning effects between the two versions, we provided participants a playground to get familiar with the QV interface and mechanism before taking the surveys. We randomized the order of the two versions.
}}

{\change{\textbf{Step 3: Filler Task}}} After both groups of participants completed their surveys on the nine societal causes, {\change{as a filler task,}} they answered a free-form text question about their thoughts on another set of societal issues unrelated to those presented in the previous stage, such as increasing funding for Medicaid, strengthening gun control, and tighten social media regulation. For a complete list of causes, please refer to the supplementary material. {\change{Using a filler task in an experiment is common in psychology~\cite{jamieson2011intervening} and HCI experiments~\cite{gould2015home, experiment2015} to prevent participants from inferring the experiment's purpose and form strategies when completing successive tasks.}} We designed this survey to prevent participants from directly connecting their survey responses with the upcoming donation tasks.

\textbf{Step 4: Donation Task} In the last step of the experiment, we need to design an incentive-compatible mechanism to elicit participants' true preferences towards the societal causes in the QV and Likert scale surveys. We designed a {\change{binding}} voluntary donation task with lottery-incentives, where their donation amount should reflect how much they truly care about the causes. In this task, to the best of their interest, they should donate more to organization A than organization B only if they care more about the cause of organization A. 

{\change{We believe a binding out-of-pocket voluntary donation was a suitable task to estimate the participants' true preferences for two reasons. First, prior works frequently estimated true preferences with voluntary donations using actual binding financial consequences in either a lab or field setting~\cite{zawojska2015re, getzner2000hypothetical, ready2010using, benz2008people, gendall2010effect}. They used participants' estimated true preferences based on actual voluntary donations as a reference to test the validity of contingent valuation methods on public or quasi-public goods. \textcite{xiao2019should} operationalized a voluntary donation with lottery-incentives in an MTurk setting. Second, voluntary donation, either online or offline, is an ecologically valid task in real-life settings. In most donation and crowdfunding websites, participants can navigate across a wide range of options to donate. It does not require specialized knowledge and is simple to conduct online and at scale.}} We now describe how our donation mechanism worked.

We showed participants a list of nine charities in a randomized order with an introduction and an official website. We selected one charity for each of the nine societal causes in the QV and Likert scale surveys via Amazon Smile. But we chose not to show their mapping to the causes explicitly to the participants to maintain as much independence between the survey responses and the donation decisions as possible. 

Participants had the chance to win \$35 as a bonus through a lottery with an odds of 1 in 70. We asked them whether they would like to donate part of this bonus to any of the nine charity groups if they were the winner. They would keep the remaining amount after the donation to themselves. While participants could donate any amount to any organization as long as the total donation value did not exceed \$35, they would want to maximize their bonus, which encouraged them to be truthful about the amount to donate. To increase participants' chance of donating a non-zero total amount {\change{\cite{meier2007subsidies}}}, the research team promised to match \$1 to each dollar they donated to an organization and execute the donation on their behalf if they won the lottery. 

\subsection{System Design}

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.7\textwidth, keepaspectratio=true]{content/image/qv-donation.png}
    \caption{
        Our QV interface design in the experiments. 
        We omitted the prompt in this figure.
        {\change{After multiple design iterations from pretest and early pilots,}}
        the final interface allows participants to vote, with real-time feedback of how the credits are allocated. 
        The progress bar design was inspired by the knapsack voting interface by~\cite{goel2015knapsack}.
    }
    \label{fig:qv_donation}
\end{figure}

We designed the QV interface through iterative-design (process detailed in \Cref{appx_qv_interface}) with the goal to reduce participants' cognitive load with visual information~\cite{oviatt2006human}. \Cref{fig:qv_donation} shows the body section of the voting panel that contained a list of options to vote on. To the left of each option, participants voted using the plus and minus buttons. Buttons for an item were automatically disabled if the number of voice credits remaining did not permit an additional vote for that item. The number of blue check or yellow cross icons next to the item represented the number of ``for'' and ``against'' votes. We provided participants a bar with percentages to the right of each option, which showed the proportion of voice credits used for that option.  In the summary panel, a progress bar showed the number of voice credits the participants have and have not used out of the total budget. We floated the summary panel at the bottom of the page at anytime to ensure visibility.

For our experimental system, we used Angular.js for the front-end, Python Flask for the back-end implementation, and MongoDB Atlas for the database. The experimental system source code is publicly available\footnote{https://github.com/a2975667/QV-app}, and so is the code for the standalone QV interface\footnote{https://github.com/hank0982/QV-app}. 

\subsection{Analysis Method -- Opinions Alignment Metric}~\label{alignment_metric}

We break our research question on ``alignment'' into two parts. First, we ask how similar these individual survey responses are to a participant's incentive-compatible behavior. Then we ask, how does QV and Likert compare overall in terms of the degree of alignment? To answer the first question, we need a metric for alignment.

We first clarify the definition of ``alignment'' in our analysis. A perfect alignment between a survey response and the same participant's donation choices requires an individual to express their preferences in survey with the same relative strength as their donation amount. More formally, it is defined as the following:

\begin{quote}
    A set of survey response $\vec{v} = (v_1, v_2, \dots, v_n) \in \mathbb{R}^n$, and a set of donation amount $\vec{d} = (d_1, d_2, \dots, d_n)\in \mathbb{R}_{+}^n$, where $n$ is the number of topics or options involved in the decision, are perfectly aligned if there exists a positive constant $k>0$ that satisfies $k\vec{v} = \vec{d}$.
\end{quote}

Notice now we represent the a participant's response in Likert scale survey, QV, and donation each with a vector of the same length. In addition, we focus the definition of alignment on the \textit{relative} strength across opinions for two reasons. First, the results from our four types of surveys (Likert, QV36, QV108, QV324) and the donation task were not on the same scale. For example, the maximum possible number of votes on a topic in QV36 was 6, while the maximum donation amount on a topic was \$35. A relative scale maps each response onto the same space. The other reason is when any two participants donated different absolute amounts, possibly due to other factors such as income level or level of education, we want to capture how they \textit{distributed} their total donation amounts. In other words, participants might donate with the \textit{same} set of preferences across topics, but with \textit{different} levels of total amount they were willing to donate. Hence, we decided to care only about the relative strength in opinions across topics.

Next, we need a metric that measures the degree of alignment. This metric needs to be monotonic with respect to the amount of discrepancy between two preference vectors in terms of the relative strength across preferences. In addition, this metric needs to be easily interpretable. Therefore, we decided to make use of the cosine similarity metric as our alignment metric and represent the difference between the survey results and the donation amount with an angle $\theta$. It is formally defined as the following:

\begin{quote}
    The cosine similarity angle $\theta$ between a set of survey response $\vec{v} = (v_1, v_2,\dots, v_n) \in \mathbb{R}^n$, and a set of donation amount $\vec{d} = (d_1, d_2,\dots, d_n) \in \mathbb{R}_{+}^n$, where $n$ is the number of topics or options involved in the decision, is calculated via $\theta = \arccos \left ( {\frac{\langle \vec{v},  \vec{d} \rangle}{\|\vec{v}\| \|\vec{d}\|}} \right )$, $\theta \in (0, \pi)$.
\end{quote}

Cosine similarity is a commonly used similarity metric 
that measures the cosine of the angle between two non-zero vectors~\cite{singhal2001modern}. Instead of reporting a value between $0$ and $2 \pi$ radians, we report the angle in degrees, allowing for a more intuitive interpretation.
Cosine similarity is monotonic with respect to the relative orientations of the two vectors, i.e., the relative strength in opinions, and does not take into account the magnitude of the vectors, i.e., absolute vote or donation amount. Two sets of perfectly align opinions will yield a cosine similarity angle of zero, while two sets of completely opposite opinions will result in an angle of 180 degree.

For the Likert group, we map the ordinal responses into a vector where the result for each topic ranges from $-2$ to $2$. For each of the three QV conditions, the vector contains the number of votes of the topics as is. Then, for each individual, we computed the cosine similarity angle between the Likert or QV vector and the absolute donation amount of the same individual. 

Once we gathered these data, we moved on to the next step where we uncovered how the cosine similarity angles compared across the Likert group and the three QV variances (QV36, QV108 and QV324). We set up a Bayesian Model with these four sets of cosine similarity angle for each condition, as described in the next subsection.

\subsection{Analysis Method -- A Bayesian Approach}
\label{exp1:The Bayesian Model}

We used Bayesian analysis to compare if the distribution of cosine similarity angle in the QV group significantly differs from that in the Likert group. {\change{While Non-Bayesian inference techniques are widely available, and in hands of an experienced statistician, can dramatically reduce time to make an inference, we use Bayesian inference techniques for the following four reasons. First, as~\textcite{kay2016researcher} point out Bayesian inference allows for accumulation of knowledge within the HCI community, where subsequent researchers can use prior outcomes as informative priors. Second, Bayesian models are transparent---researchers foreground all the assumptions in the model. There are no assumptions  (e.g., Normality assumptions for the $t$-test) that need to be cross-validated with the data. The Bayesian model transparency avoids the intentionality pitfall in null hypothesis significance testing (NHST)~\cite{ioannidis2005most, kruschke2010bayesian}---the choice of critical $p$-value is dependent on researchers' intentions, including sample size adjustments and pre-selection of hypotheses. Third,~\textcite{kay2016researcher} suggest a that Bayesian formulation shifts the conversation focus from ``did it work" to ``how strong is the effect." A posterior probability distribution of estimated effect size in a Bayesian analysis plays a critical role when stakeholders perform a cost-benefit analysis on deciding which surveys to use~\cite{kay2016researcher}. While NHST can estimate the effect size (a point estimate) and a confidence interval, the NHST logic relegates this information as secondary to the $p$-value and under-emphasizes it. Finally, as~\textcite{McElreath2015} points out, since Bayesian priors use maximum entropy distributions (e.g., Normal, Gamma distributions), the inference is the \textit{most conservative} given the evidence.}}
 
Now, we discuss our Bayesian formulation. There is one outcome variable: $\theta_{i \mid j}$, the cosine similarity angle between a survey response vector and a donation amount vector of each participant $i$  under each of the four experimental conditions $j$: Likert, and three QV conditions (with 36, 108 and 324 credits). In summary, we aim to fit a distribution for the mean (i.e. the expected) angle between responses from each survey method and the donation amount. Then we compare how different the four distributions are. 

In a Bayesian formulation, we need to define a likelihood function to model the cosine similarity under each condition. In general, this is a parametric formulation, and consistent with~\textcite{McElreath2015}. The likelihood function represents the modeler's view of the data, and not a claim about the world. The likelihood function is often parametric and we treat each model parameter as a random variable, drawn from a distribution (its prior). Typically, these priors are ``weakly informative''---conservative priors which allow for all possible values of the parameter but chosen in a manner that promotes fast convergence.


We use a Student-t distribution to characterize the mean (i.e. the expected) angle in all four survey conditions (Likert and the three QV conditions). A Student-t, unlike a Normal distribution, is heavy-tailed, in the sense that the Student-t distribution doesn't fall off as quickly as does a Normal distribution and will thus be able to better account for outliers in the data {\change{\cite{kay2016researcher}.}} The Student-t distribution has three parameters: the degrees of freedom ($\nu$), the experimental condition dependent mean ($\mu_j$) and scale ($\sigma_j$).  These parameters are random variables and we need to define their priors. Since our goal is to model the \textit{average} angle, the fact that the Student-t is unbounded while the angle $\theta \in [0, \pi]$ is bounded is unimportant.

\begin{align}
  \theta_{i \mid j} \sim & \mathrm{Student-t}(\nu, \mu_j, \sigma_j),   & \text{likelihood function to model donation} \label{eq:bayesian formulation} \\
  \nu \sim & 1 + exp(\lambda), & \text{degrees of freedom} \\
  \mu_j \sim & N(M_0, \sigma_0), & \text{modal angle in condition } j \\
  \sigma_j \sim & \Gamma(\alpha, \beta), & \text{scale parameter for condition } j
\end{align}
 
\Cref{eq:bayesian formulation} describes that the response $\theta_{i \mid j}$ of each group $j$ is modeled as a $\mathrm{Student-t}$ distribution with mode $\mu_j$,  scale $\sigma_j$ and with $\nu$ degrees of freedom. Next, we explain the model parameters.


\begin{description}
    \item[Degrees of Freedom:] We draw the degrees of freedom $\nu$ from a shifted exponential distribution, to ensure $\nu \geq 1$; $\nu=\infty$, corresponds to a Normal distribution assumption.
    \item[Modal contribution $\mu_j$ in each condition $j$:] The mode $\mu_j$ corresponding to each group is drawn from a Normally distributed random variables with constant mean $M_0$ and variance $\sigma_0$. 
    \item[Scale $\sigma_j$ of each condition $j$:]  The scale $\sigma_j$ of the likelihood function is drawn from a Gamma distribution $\Gamma(\alpha, \beta)$, with mode $\alpha$ and scale $\beta$; this prior on $\sigma_j$ ensures that $\sigma_j > 0$. 
    \item[Constants:] The constants $M_0, \sigma_0, \alpha, \beta$ are set so that the priors are generous but weakly informative so that despite exploring all possible values, we ensure rapid MCMC convergence.
\end{description}

We performed the Bayesian analysis using PyMC3~\cite{salvatier2016probabilistic}, 
a popular Bayesian inference framework. We used one of the common computational techniques for Bayesian inference, Markov Chain Monte Carlo (MCMC), a stochastic sampling technique. It samples the posterior distribution $P(\theta | D)$, the distribution functions of the parameters in the likelihood function given the data observations $D$. 
